<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Friends don't let friends export to CSV | Robin's blog</title>
<meta name=keywords content="data,software engineering"><meta name=description content="I worked for a few years in the intersection between data science and software
engineering. On the whole, it was a really enjoyable time and I&rsquo;d like to have
the chance to do so again at some point. One of the least enjoyable experiences
from that time was to deal with big CSV exports. Unfortunately, this file format
is still very common in the data science space. It is easy to understand why &ndash;
it seems to be ubiquitous, present everywhere, it&rsquo;s human-readable, it&rsquo;s less
verbose than options like JSON and XML, it&rsquo;s super easy to produce from almost
any tool. What&rsquo;s not to like?"><meta name=author content="Robin Kåveland"><link rel=canonical href=https://kaveland.no/posts/2024-03-10-friends-dont-let-friends-export-csv/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://kaveland.no/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://kaveland.no/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://kaveland.no/favicon-32x32.png><link rel=apple-touch-icon href=https://kaveland.no/apple-touch-icon.png><link rel=mask-icon href=https://kaveland.no/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://kaveland.no/posts/2024-03-10-friends-dont-let-friends-export-csv/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script defer data-domain=kaveland.no src=https://plausible.io/js/script.js></script><meta property="og:title" content="Friends don't let friends export to CSV"><meta property="og:description" content="I worked for a few years in the intersection between data science and software
engineering. On the whole, it was a really enjoyable time and I&rsquo;d like to have
the chance to do so again at some point. One of the least enjoyable experiences
from that time was to deal with big CSV exports. Unfortunately, this file format
is still very common in the data science space. It is easy to understand why &ndash;
it seems to be ubiquitous, present everywhere, it&rsquo;s human-readable, it&rsquo;s less
verbose than options like JSON and XML, it&rsquo;s super easy to produce from almost
any tool. What&rsquo;s not to like?"><meta property="og:type" content="article"><meta property="og:url" content="https://kaveland.no/posts/2024-03-10-friends-dont-let-friends-export-csv/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-04T00:00:00+00:00"><meta property="og:site_name" content="Robin's blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Friends don't let friends export to CSV"><meta name=twitter:description content="I worked for a few years in the intersection between data science and software
engineering. On the whole, it was a really enjoyable time and I&rsquo;d like to have
the chance to do so again at some point. One of the least enjoyable experiences
from that time was to deal with big CSV exports. Unfortunately, this file format
is still very common in the data science space. It is easy to understand why &ndash;
it seems to be ubiquitous, present everywhere, it&rsquo;s human-readable, it&rsquo;s less
verbose than options like JSON and XML, it&rsquo;s super easy to produce from almost
any tool. What&rsquo;s not to like?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kaveland.no/posts/"},{"@type":"ListItem","position":2,"name":"Friends don't let friends export to CSV","item":"https://kaveland.no/posts/2024-03-10-friends-dont-let-friends-export-csv/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Friends don't let friends export to CSV","name":"Friends don\u0027t let friends export to CSV","description":"I worked for a few years in the intersection between data science and software engineering. On the whole, it was a really enjoyable time and I\u0026rsquo;d like to have the chance to do so again at some point. One of the least enjoyable experiences from that time was to deal with big CSV exports. Unfortunately, this file format is still very common in the data science space. It is easy to understand why \u0026ndash; it seems to be ubiquitous, present everywhere, it\u0026rsquo;s human-readable, it\u0026rsquo;s less verbose than options like JSON and XML, it\u0026rsquo;s super easy to produce from almost any tool. What\u0026rsquo;s not to like?\n","keywords":["data","software engineering"],"articleBody":"I worked for a few years in the intersection between data science and software engineering. On the whole, it was a really enjoyable time and I’d like to have the chance to do so again at some point. One of the least enjoyable experiences from that time was to deal with big CSV exports. Unfortunately, this file format is still very common in the data science space. It is easy to understand why – it seems to be ubiquitous, present everywhere, it’s human-readable, it’s less verbose than options like JSON and XML, it’s super easy to produce from almost any tool. What’s not to like?\nEdited section: In hindsight, the title of this post is perhaps too controversial, and makes it too easy to miss the point. If you’re exporting CSV to end users, to be consumed in spreadsheets, I too would probably just go for CSV. Please consider something else if you can reasonably expect your users to want to work with the data with tools like spark, pandas, R, polars or if it matters to you that people can make robust integrations against your data export. This post was discussed in this hacker news thread.\nCSV is (usually) underspecified Sure, RFC4180 exists, but in practice, CSV is actually a family of file formats that have been in use since the early 1970s. If a file has a .csv-suffix, you don’t actually have all of the information that you require in order to parse it correctly, it’s severely underspecified and in practice, you’ll need to open the file and look at it in order to load the data into a programming environment. Here are some issues you’re likely to encounter in the wild:\nWhat does missing data look like? The empty string, NaN, 0, 1/1-1970, null, nil, NULL, \\0? What date format will you need to parse? What does 5/5/12 mean? How multiline data has been written? Does it use quotation marks, properly escape those inside multiline strings, or maybe it just expects you to count the delimiter and by the way can delimiters occur inside bare strings? One of the infurating things about the format is that things often break in ways that tools can’t pick up and tell you about, leading to data corruption instead of an error. I remember spending hours trying to identify an issue that caused columns to “shift” around 80% into a 40GB CSV file, and let me tell you, that just isn’t fun. In the end, this came down to a problem that needed to be fixed in the source, which was producing invalid CSVs, and I wasn’t able to work around the issue using spark on my end, because I needed to process and fix the records serially to avoid the issue and at that point, why would I want to use a cluster anyway.\nCSV files have terrible compression and performance If you start building something that’s producing or consuming CSV files, you quickly discover that it’s annoying to ship them around. Storage is cheap, but performance matters and shipping around 50GB files take a lot of time.\nThe next logical move from there is to tack on compression and end up with .csv.gz or .csv.zip or some other format that’s essentially still CSV. For the type of data that we use CSV for, this often leads to amazing compression, perhaps a factor of 10 or 20. Unfortunately, this leads to vastly more expensive loading and querying of the files. The worst case is if you need to query for something near the tail end of the file – the file now must be read serially, there’s no way to seek to somewhere close to where the data must be. This is perhaps fine if you’re going to read all of the records every time the file is loaded, but even then you’re still going to be paying with performance, it’ll take a long time to uncompress big files and materialize the data in memory. All of the textual data in the file must be uncompressed and moved to RAM.\nAnother issue is related to data types. Most CSV readers will read \"2024-03-24T21:39:23.930074+01:00\" into a 32 character long string, which may be well in excess of 32 bytes, depending on how a string is represented in memory in your programming environment (in Python, for example, this string weighs 81 bytes). Then it’ll hopefully be parsed into an 8 byte integer and one byte timezone offset, and we do the reverse when we serialize again. This is a lot of wasted effort, compared to just storing the 8 byte integer and the one byte offset to begin with. If you work in an environment like pandas, where you want to materialize the whole data set in memory before you process it, this explosion in size quickly limits how much data you can actually process at all, before reaching for a more complex tool, like spark.\nNumerical columns may also be ambigious, there’s no way to know if you can read a numerical column into an integral data type, or if you need to reach for a float without first reading all the records. There’s tons of variations in how we serialize and deserialize bools too. Chances are pretty good that you’ll spend quite a while looking at your file before you can parse it correctly.\nThere’s a better way Actually, there are many file formats that are more suitable to working with tabular data. I’m a big fan of Apache Parquet as a good default. You give up human readable files, but what you gain in return is incredibly valuable:\nSelf-describing files that contain their own schema so data is loaded with the right data types automatically. Really good compression properties, competetive with .csv.gz when run-length encoding works well for the data. Effortless and super fast loading of data into memory, since the files contain their own schema, you do not need to sit down and work out data types before loading, you simply ask the file. You only pay for the columns that you actually load, unused columns do not need to be read from disk/network. This means you can offer incredibly wide tables and let the user specify columns on loading from the network, rather than making one export for each permutation of columns that’s requested. Support for complex data types, like record types or arrays. There are other binary data formats that also work really well, but for the use case where people often reach for CSV, parquet is easily my favorite. If you absolutely must do streaming writes with record-oriented data, perhaps you should look into something like Avro instead.\nBacking it up with numbers: Compression I’ve claimed that parquet is a vastly superior file format to CSV, both when it comes to compression, convenience and performance so let’s do some experiments to back that up. I’m going to be presenting some file size and timing information and I’ll be using pandas and polars for my examples, the code is written run using IPython.\nTo get some numbers, I’ve downloaded this football dataset from kaggle, containing 44269 football match scores from 1995 to 2020. This is a 3.7MB CSV file, containing 1 date column, a country column, team names columns and other than that primarily small ints, a total of 14 columns.\nLet’s take a look at what kind of compression we can achieve by transforming it into .csv.gz and .pq first. This is the code I used to load the file and transform it into parquet:\ndtype = { 'Team 1': 'category', 'FT': 'category', 'HT': 'category', 'Team 2': 'category', 'Round': 'int8', 'Year': 'int16', 'Country': 'category', 'FT Team 1': 'int8', 'FT Team 2': 'int8', 'HT Team 1': 'int8', 'HT Team 2': 'int8', 'GGD': 'int8', 'Team 1 (pts)': 'int8', 'Team 2 (pts)': 'int8', } date_fmt = '(%a) %d %b %Y (W%W)' df = pd.read_csv('BIG FIVE 1995-2019.csv', dtype=dtype).assign( Date=lambda df: pd.to_datetime(df.Date, format=date_fmt) ) df.to_parquet('football.pq') Notice how the date column has a somewhat annoying format that looks like this: \"(Sat) 19 Aug 1995 (W33)\" – it already took me quite a while to find the correct format string to parse that, which I wouldn’t have had to do if this was a binary file format with native support for timestamps. Let’s take a look at how the file sizes came out:\ndu -hs BIG\\ FIVE\\ 1995-2019.csv BIG\\ FIVE\\ 1995-2019.csv.gz football.pq 3.7M\tBIG FIVE 1995-2019.csv 472K\tBIG FIVE 1995-2019.csv.gz 376K\tfootball.pq In this case, the .pq ended up being smaller than the .csv.gz.\nBacking it up with numbers: Performance Let’s check the relative time difference it takes to load both into memory:\n%timeit df = pd.read_parquet(\"football.pq\") # 2.26 ms ± 60.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %%timeit df = pd.read_csv(\"BIG FIVE 1995-2019.csv.gz\", dtype=dtype).assign( Date=lambda df: pd.to_datetime(df.Date, format=date_fmt) ) # 47.1 ms ± 380 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) %%timeit df = pd.read_csv(\"BIG FIVE 1995-2019.csv\", dtype=dtype).assign( Date=lambda df: pd.to_datetime(df.Date, format=date_fmt) ) # 41.8 ms ± 880 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) To summarize, in this case:\nThe .pq file is smaller than the CSV by a factor of about 10, and it’s also smaller than the .csv.gz. The .pq file is faster to load by a factor of about 25 compared to the .csv.gz and a factor of around 19 compared to the CSV. I can load the .pq file with a single instruction, not knowing anything about the file up front, whereas I need to figure out the data types and date formats up front for both the other options. Just to really drive the point home, let’s find out how long it takes to check how many matches Newcaste United FC have won in this data set with a query using polars, and run it both against the CSV and against the parquet.\nhome_wins = ( (pl.col(\"Team 1\") == \"Newcastle United FC \") \u0026 (pl.col(\"FT Team 1\") \u003e pl.col(\"FT Team 2\")) ) away_wins = ( (pl.col(\"Team 2\") == \"Newcastle United FC \") \u0026 (pl.col(\"FT Team 2\") \u003e pl.col(\"FT Team 1\")) ) where = home_wins | away_wins count = pl.col(\"Team 1\").count().alias(\"wins\") %timeit pl.scan_csv(\"BIG FIVE 1995-2019.csv\" ).filter(where).select(count).collect() # 1.71 ms ± 37.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %timeit pl.read_csv(\"BIG FIVE 1995-2019.csv.gz\" ).lazy().filter(where).select(count).collect() # 9.44 ms ± 29.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %timeit pl.scan_parquet(\"football.pq\" ).filter(where).select(count).collect() # 647 µs ± 6.37 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Note that polars does not allow us to lazy-read a compressed CSV, it needs to unpack the whole thing anyway, so in order to run a lazy query we need to read the whole file. With the parquet file, we’re now only reading 4 columns and leaving the majority of them on disk and it’s significantly faster than the CSV file that occupies much more disk space.\nConclusion Of course, we can’t conclude that you should never export to CSV. If your users are just going to try to find the quickest way to turn your data into CSV anyway, there’s no reason why you shouldn’t deliver that. But it’s a super fragile file format to use for anything serious like data integration between systems, so stick with something that at the very least has a schema and is more efficient to work with.\n","wordCount":"1915","inLanguage":"en","datePublished":"2024-03-24T00:00:00Z","dateModified":"2024-04-04T00:00:00Z","author":{"@type":"Person","name":"Robin Kåveland"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kaveland.no/posts/2024-03-10-friends-dont-let-friends-export-csv/"},"publisher":{"@type":"Organization","name":"Robin's blog","logo":{"@type":"ImageObject","url":"https://kaveland.no/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kaveland.no/ accesskey=h title="Robin's blog (Alt + H)">Robin's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kaveland.no/about/ title=about><span>about</span></a></li><li><a href=https://kaveland.no/projects/ title=projects><span>projects</span></a></li><li><a href=https://kaveland.no/eugene/ title=eugene><span>eugene</span></a></li><li><a href=https://kaveland.no/thumper/ title=thumper><span>thumper</span></a></li><li><a href=https://kaveland.no/tags/ title=tags><span>tags</span></a></li><li><a href=https://kaveland.no/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kaveland.no/>Home</a>&nbsp;»&nbsp;<a href=https://kaveland.no/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Friends don't let friends export to CSV</h1><div class=post-meta><span title='2024-03-24 00:00:00 +0000 UTC'>March 24, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1915 words&nbsp;·&nbsp;Robin Kåveland&nbsp;|&nbsp;<a href=https://github.com/kaaveland/kaaveland.github.io/content/posts/2024-03-10-friends-dont-let-friends-export-csv.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#csv-is-usually-underspecified>CSV is (usually) underspecified</a></li><li><a href=#csv-files-have-terrible-compression-and-performance>CSV files have terrible compression and performance</a></li><li><a href=#theres-a-better-way>There&rsquo;s a better way</a></li><li><a href=#backing-it-up-with-numbers-compression>Backing it up with numbers: Compression</a></li><li><a href=#backing-it-up-with-numbers-performance>Backing it up with numbers: Performance</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>I worked for a few years in the intersection between data science and software
engineering. On the whole, it was a really enjoyable time and I&rsquo;d like to have
the chance to do so again at some point. One of the least enjoyable experiences
from that time was to deal with big CSV exports. Unfortunately, this file format
is still very common in the data science space. It is easy to understand why &ndash;
it seems to be ubiquitous, present everywhere, it&rsquo;s human-readable, it&rsquo;s less
verbose than options like JSON and XML, it&rsquo;s super easy to produce from almost
any tool. What&rsquo;s not to like?</p><p><strong>Edited section</strong>: In hindsight, the title of this post is perhaps too
controversial, and makes it too easy to miss the point. If you&rsquo;re exporting CSV
to end users, to be consumed in spreadsheets, I too would probably just go for
CSV. Please consider something else if you can reasonably expect your users to
want to work with the data with tools like spark, pandas, R, polars or if it
matters to you that people can make robust integrations against your data export.
This post was discussed in this <a href="https://news.ycombinator.com/item?id=39814334">hacker news</a>
thread.</p><h2 id=csv-is-usually-underspecified>CSV is (usually) underspecified<a hidden class=anchor aria-hidden=true href=#csv-is-usually-underspecified>#</a></h2><p>Sure, <a href=https://datatracker.ietf.org/doc/html/rfc4180>RFC4180</a> <em>exists</em>, but in
practice, CSV is actually a family of file formats that have been in use since
the early 1970s. If a file has a <code>.csv</code>-suffix, you don&rsquo;t actually have all of
the information that you require in order to parse it correctly, it&rsquo;s severely
underspecified and in practice, you&rsquo;ll need to open the file and look at it
in order to load the data into a programming environment. Here are some issues
you&rsquo;re likely to encounter in the wild:</p><ul><li>What does missing data look like? The empty string, <code>NaN</code>, <code>0</code>, <code>1/1-1970</code>,
<code>null</code>, <code>nil</code>, <code>NULL</code>, <code>\0</code>?</li><li>What date format will you need to parse? What does <code>5/5/12</code> mean?</li><li>How multiline data has been written? Does it use quotation marks, properly
escape those inside multiline strings, or maybe it just expects you to count
the delimiter and by the way can delimiters occur inside bare strings?</li></ul><p>One of the infurating things about the format is that things often break in ways
that tools can&rsquo;t pick up and tell you about, leading to data corruption instead
of an error. I remember spending hours trying to identify an issue that caused
columns to &ldquo;shift&rdquo; around 80% into a 40GB CSV file, and let me tell you, that
just isn&rsquo;t fun. In the end, this came down to a problem that needed to be fixed
in the source, which was producing invalid CSVs, and I wasn&rsquo;t able to work
around the issue using spark on my end, because I needed to process and fix the
records serially to avoid the issue and at that point, why would I want to use a
cluster anyway.</p><h2 id=csv-files-have-terrible-compression-and-performance>CSV files have terrible compression and performance<a hidden class=anchor aria-hidden=true href=#csv-files-have-terrible-compression-and-performance>#</a></h2><p>If you start building something that&rsquo;s producing or consuming CSV files, you
quickly discover that it&rsquo;s annoying to ship them around. Storage is cheap, but
performance matters and shipping around 50GB files take a lot of time.</p><p>The next logical move from there is to tack on compression and end up with
<code>.csv.gz</code> or <code>.csv.zip</code> or some other format that&rsquo;s essentially still CSV.
For the type of data that we use CSV for, this often leads to amazing
compression, perhaps a factor of 10 or 20. Unfortunately, this leads to
vastly more expensive loading and querying of the files. The worst case is if
you need to query for something near the tail end of the file &ndash; the file now
<em>must</em> be read serially, there&rsquo;s no way to seek to somewhere close to where
the data must be. This is perhaps fine if you&rsquo;re going to read all of the
records every time the file is loaded, but even then you&rsquo;re still going to be
paying with performance, it&rsquo;ll take a long time to uncompress big files and
materialize the data in memory. All of the textual data in the file must be
uncompressed and moved to RAM.</p><p>Another issue is related to data types. Most CSV readers will read
<code>"2024-03-24T21:39:23.930074+01:00"</code> into a 32 character long string, which may
be well in excess of 32 bytes, depending on how a string is represented in
memory in your programming environment (in Python, for example, this string
weighs 81 bytes). Then it&rsquo;ll hopefully be parsed into an 8 byte integer and
one byte timezone offset, and we do the reverse when we serialize again. This is
a lot of wasted effort, compared to just storing the 8 byte integer and
the one byte offset to begin with. If you work in an environment like pandas,
where you want to materialize the whole data set in memory before you process
it, this explosion in size quickly limits how much data you can actually process
at all, before reaching for a more complex tool, like spark.</p><p>Numerical columns may also be ambigious, there&rsquo;s no way to know if you can read
a numerical column into an integral data type, or if you need to reach for a
float without first reading all the records. There&rsquo;s tons of variations in how
we serialize and deserialize bools too. Chances are pretty good that you&rsquo;ll
spend quite a while looking at your file before you can parse it correctly.</p><h2 id=theres-a-better-way>There&rsquo;s a better way<a hidden class=anchor aria-hidden=true href=#theres-a-better-way>#</a></h2><p>Actually, there are many file formats that are more suitable to working with
tabular data. I&rsquo;m a big fan of
<a href=https://en.wikipedia.org/wiki/Apache_Parquet>Apache Parquet</a> as a good default.
You give up human readable files, but what you gain in return is incredibly
valuable:</p><ul><li>Self-describing files that contain their own schema so data is loaded with
the right data types automatically.</li><li>Really good compression properties, competetive with <code>.csv.gz</code> when run-length
encoding works well for the data.</li><li>Effortless and super fast loading of data into memory, since the files contain
their own schema, you do not need to sit down and work out data types before
loading, you simply ask the file.</li><li>You only pay for the columns that you actually load, unused columns do not
need to be read from disk/network. This means you can offer incredibly wide
tables and let the user specify columns on loading from the network, rather
than making one export for each permutation of columns that&rsquo;s requested.</li><li>Support for complex data types, like record types or arrays.</li></ul><p>There are other binary data formats that also work really well, but for the use
case where people often reach for CSV, parquet is easily my favorite. If you
absolutely must do streaming writes with record-oriented data, perhaps you
should look into something like <a href=https://avro.apache.org/>Avro</a> instead.</p><h2 id=backing-it-up-with-numbers-compression>Backing it up with numbers: Compression<a hidden class=anchor aria-hidden=true href=#backing-it-up-with-numbers-compression>#</a></h2><p>I&rsquo;ve claimed that parquet is a vastly superior file format to CSV, both when it
comes to compression, convenience and performance so let&rsquo;s do some experiments
to back that up. I&rsquo;m going to be presenting some file size and timing
information and I&rsquo;ll be using <a href=https://pandas.pydata.org/>pandas</a> and
<a href=https://pola.rs/>polars</a> for my examples, the code is written run using
IPython.</p><p>To get some numbers, I&rsquo;ve downloaded
<a href=https://www.kaggle.com/datasets/hikne707/big-five-european-soccer-leagues/data>this football dataset</a>
from kaggle, containing 44269 football match scores from 1995 to 2020. This is a
3.7MB CSV file, containing 1 date column, a country column, team names columns
and other than that primarily small ints, a total of 14 columns.</p><p>Let&rsquo;s take a look at what kind of compression we can achieve by transforming it
into <code>.csv.gz</code> and <code>.pq</code> first. This is the code I used to load the file and
transform it into parquet:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dtype</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Team 1&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;FT&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;HT&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Team 2&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Round&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span> <span class=s1>&#39;Year&#39;</span><span class=p>:</span> <span class=s1>&#39;int16&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Country&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;FT Team 1&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;FT Team 2&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;HT Team 1&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;HT Team 2&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;GGD&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Team 1 (pts)&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Team 2 (pts)&#39;</span><span class=p>:</span> <span class=s1>&#39;int8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>date_fmt</span> <span class=o>=</span> <span class=s1>&#39;(</span><span class=si>%a</span><span class=s1>) </span><span class=si>%d</span><span class=s1> %b %Y (W%W)&#39;</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;BIG FIVE 1995-2019.csv&#39;</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>Date</span><span class=o>=</span><span class=k>lambda</span> <span class=n>df</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>to_datetime</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>Date</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=n>date_fmt</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>to_parquet</span><span class=p>(</span><span class=s1>&#39;football.pq&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p>Notice how the date column has a somewhat annoying format that looks like this:
<code>"(Sat) 19 Aug 1995 (W33)"</code> &ndash; it already took me quite a while to find the
correct format string to parse that, which I wouldn&rsquo;t have had to do if this was
a binary file format with native support for timestamps. Let&rsquo;s take a look at
how the file sizes came out:</p><pre tabindex=0><code class=language-shellsession data-lang=shellsession>du -hs BIG\ FIVE\ 1995-2019.csv BIG\ FIVE\ 1995-2019.csv.gz football.pq
3.7M	BIG FIVE 1995-2019.csv
472K	BIG FIVE 1995-2019.csv.gz
376K	football.pq
</code></pre><p>In this case, the <code>.pq</code> ended up being smaller than the <code>.csv.gz</code>.</p><h2 id=backing-it-up-with-numbers-performance>Backing it up with numbers: Performance<a hidden class=anchor aria-hidden=true href=#backing-it-up-with-numbers-performance>#</a></h2><p>Let&rsquo;s check the relative time difference it takes to load both into memory:</p><pre tabindex=0><code class=language-ipython data-lang=ipython>%timeit df = pd.read_parquet(&#34;football.pq&#34;)
# 2.26 ms ± 60.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%%timeit
df = pd.read_csv(&#34;BIG FIVE 1995-2019.csv.gz&#34;, dtype=dtype).assign(
    Date=lambda df: pd.to_datetime(df.Date, format=date_fmt)
)
# 47.1 ms ± 380 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%%timeit
df = pd.read_csv(&#34;BIG FIVE 1995-2019.csv&#34;, dtype=dtype).assign(
    Date=lambda df: pd.to_datetime(df.Date, format=date_fmt)
)
# 41.8 ms ± 880 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><p>To summarize, in this case:</p><ul><li>The <code>.pq</code> file is smaller than the CSV by a factor of about 10, and it&rsquo;s
also smaller than the <code>.csv.gz</code>.</li><li>The <code>.pq</code> file is faster to load by a factor of about 25 compared to the
<code>.csv.gz</code> and a factor of around 19 compared to the CSV.</li><li>I can load the <code>.pq</code> file with a single instruction, not knowing anything
about the file up front, whereas I need to figure out the data types and
date formats up front for both the other options.</li></ul><p>Just to really drive the point home, let&rsquo;s find out how long it takes to
check how many matches Newcaste United FC have won in this data set with a query
using polars, and run it both against the CSV and against the parquet.</p><pre tabindex=0><code class=language-ipython data-lang=ipython>home_wins = (
        (pl.col(&#34;Team 1&#34;) == &#34;Newcastle United FC &#34;) &amp;
        (pl.col(&#34;FT Team 1&#34;) &gt; pl.col(&#34;FT Team 2&#34;))
)
away_wins = (
        (pl.col(&#34;Team 2&#34;) == &#34;Newcastle United FC &#34;) &amp;
        (pl.col(&#34;FT Team 2&#34;) &gt; pl.col(&#34;FT Team 1&#34;))
)
where = home_wins | away_wins
count = pl.col(&#34;Team 1&#34;).count().alias(&#34;wins&#34;)


%timeit pl.scan_csv(&#34;BIG FIVE 1995-2019.csv&#34;
  ).filter(where).select(count).collect()
# 1.71 ms ± 37.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
%timeit pl.read_csv(&#34;BIG FIVE 1995-2019.csv.gz&#34;
  ).lazy().filter(where).select(count).collect()
# 9.44 ms ± 29.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
%timeit pl.scan_parquet(&#34;football.pq&#34;
  ).filter(where).select(count).collect()
# 647 µs ± 6.37 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</code></pre><p>Note that polars does not allow us to lazy-read a compressed CSV, it needs to
unpack the whole thing anyway, so in order to run a lazy query we need to read
the whole file. With the parquet file, we&rsquo;re now only reading 4 columns and
leaving the majority of them on disk and it&rsquo;s significantly faster than the CSV
file that occupies much more disk space.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Of course, we can&rsquo;t conclude that you should <em>never</em> export to CSV. If your
users are just going to try to find the quickest way to turn your data into CSV
anyway, there&rsquo;s no reason why you shouldn&rsquo;t deliver that. But it&rsquo;s a super
fragile file format to use for anything serious like data integration between
systems, so stick with something that at the very least has a schema and is more
efficient to work with.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://kaveland.no/tags/data/>Data</a></li><li><a href=https://kaveland.no/tags/software-engineering/>Software Engineering</a></li></ul><nav class=paginav><a class=prev href=https://kaveland.no/posts/2024-04-04-testcase-for-foreign-keys/><span class=title>« Prev</span><br><span>How to test for missing indexes on foreign keys</span>
</a><a class=next href=https://kaveland.no/posts/2024-03-10-testing-transactions-that-commit/><span class=title>Next »</span><br><span>Isolating integration tests that commit transactions</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on x" href="https://x.com/intent/tweet/?text=Friends%20don%27t%20let%20friends%20export%20to%20CSV&amp;url=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f&amp;hashtags=data%2csoftwareengineering"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f&amp;title=Friends%20don%27t%20let%20friends%20export%20to%20CSV&amp;summary=Friends%20don%27t%20let%20friends%20export%20to%20CSV&amp;source=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f&title=Friends%20don%27t%20let%20friends%20export%20to%20CSV"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on whatsapp" href="https://api.whatsapp.com/send?text=Friends%20don%27t%20let%20friends%20export%20to%20CSV%20-%20https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on telegram" href="https://telegram.me/share/url?text=Friends%20don%27t%20let%20friends%20export%20to%20CSV&amp;url=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Friends don't let friends export to CSV on ycombinator" href="https://news.ycombinator.com/submitlink?t=Friends%20don%27t%20let%20friends%20export%20to%20CSV&u=https%3a%2f%2fkaveland.no%2fposts%2f2024-03-10-friends-dont-let-friends-export-csv%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://kaveland.no/>Robin's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>