<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>No-ops linux part 3: It puts the data in the pond. Nightly. | Robin's blog</title>
<meta name=keywords content="cloud,linux,ops,cdn,duckdb,caddy,ansible"><meta name=description content="This post is part of the series on no-ops linux deployment. The first post covered local development of linux server configuration and essential configuration. The previous installment covers a janky podman installation and configures a reverse proxy to send traffic to a simple container deployment. This is the final post. It covers a more challenging deployment with jobs and rolling restarts, and discusses the strengths and weaknesses of this approach to hosting.
After the previous post, we know how to deploy a container that requires absolutely no configuration and restarts almost instantly. Most of the applications I work on in my daytime job aren&rsquo;t like that. Let&rsquo;s take a look at a more complex example."><meta name=author content="Robin K√•veland"><link rel=canonical href=https://kaveland.no/posts/2025-05-14-fire-and-forget-linux-p3/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://kaveland.no/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://kaveland.no/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://kaveland.no/favicon-32x32.png><link rel=apple-touch-icon href=https://kaveland.no/apple-touch-icon.png><link rel=mask-icon href=https://kaveland.no/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://kaveland.no/posts/2025-05-14-fire-and-forget-linux-p3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script defer data-domain=kaveland.no src=https://plausible.io/js/script.js></script><meta property="og:title" content="No-ops linux part 3: It puts the data in the pond. Nightly."><meta property="og:description" content="This post is part of the series on no-ops linux deployment. The first post covered local development of linux server configuration and essential configuration. The previous installment covers a janky podman installation and configures a reverse proxy to send traffic to a simple container deployment. This is the final post. It covers a more challenging deployment with jobs and rolling restarts, and discusses the strengths and weaknesses of this approach to hosting.
After the previous post, we know how to deploy a container that requires absolutely no configuration and restarts almost instantly. Most of the applications I work on in my daytime job aren&rsquo;t like that. Let&rsquo;s take a look at a more complex example."><meta property="og:type" content="article"><meta property="og:url" content="https://kaveland.no/posts/2025-05-14-fire-and-forget-linux-p3/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-14T18:00:00+00:00"><meta property="article:modified_time" content="2025-05-14T18:00:00+00:00"><meta property="og:site_name" content="Robin's blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="No-ops linux part 3: It puts the data in the pond. Nightly."><meta name=twitter:description content="This post is part of the series on no-ops linux deployment. The first post covered local development of linux server configuration and essential configuration. The previous installment covers a janky podman installation and configures a reverse proxy to send traffic to a simple container deployment. This is the final post. It covers a more challenging deployment with jobs and rolling restarts, and discusses the strengths and weaknesses of this approach to hosting.
After the previous post, we know how to deploy a container that requires absolutely no configuration and restarts almost instantly. Most of the applications I work on in my daytime job aren&rsquo;t like that. Let&rsquo;s take a look at a more complex example."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kaveland.no/posts/"},{"@type":"ListItem","position":2,"name":"No-ops linux part 3: It puts the data in the pond. Nightly.","item":"https://kaveland.no/posts/2025-05-14-fire-and-forget-linux-p3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"No-ops linux part 3: It puts the data in the pond. Nightly.","name":"No-ops linux part 3: It puts the data in the pond. Nightly.","description":"This post is part of the series on no-ops linux deployment. The first post covered local development of linux server configuration and essential configuration. The previous installment covers a janky podman installation and configures a reverse proxy to send traffic to a simple container deployment. This is the final post. It covers a more challenging deployment with jobs and rolling restarts, and discusses the strengths and weaknesses of this approach to hosting.\nAfter the previous post, we know how to deploy a container that requires absolutely no configuration and restarts almost instantly. Most of the applications I work on in my daytime job aren\u0026rsquo;t like that. Let\u0026rsquo;s take a look at a more complex example.\n","keywords":["cloud","linux","ops","cdn","duckdb","caddy","ansible"],"articleBody":"This post is part of the series on no-ops linux deployment. The first post covered local development of linux server configuration and essential configuration. The previous installment covers a janky podman installation and configures a reverse proxy to send traffic to a simple container deployment. This is the final post. It covers a more challenging deployment with jobs and rolling restarts, and discusses the strengths and weaknesses of this approach to hosting.\nAfter the previous post, we know how to deploy a container that requires absolutely no configuration and restarts almost instantly. Most of the applications I work on in my daytime job aren‚Äôt like that. Let‚Äôs take a look at a more complex example.\nIntroducing the kollektivkart ‚ú®data pond‚ú® kollektivkart pulls data from Google BigQuery to S3-compatible storage, runs some DuckDB queries on it and shows it in a map (somewhat simplified). The data set it pulls from is open data, and documented at data.entur.no. The source code is freely available, so you can steal it if you wish.\nThis service could easily use a local disk. It pulls down around 20GB of data from BigQuery as partitioned parquet datasets. After crunching everything I find interesting, it occupies around 40GB of space, including around 700 million rows with 21 columns of raw data, 400 million rows with 18 columns of refined data and 6 million rows of aggregated data that can be visualized. This will work fine on even a cheap cloud virtual machine. What a time to be alive.\nIt is incredibly nice to make the server stateless, to the degree that I can. This ensures that I can quickly and easily replace the machine with another one. So, that‚Äôs what I‚Äôll do. The jobs and the webapp can both read/write from s3:// paths or from a local disk. We‚Äôll do it on hard-mode and configure it to work with S3, that way, we can move the app to new servers without having to copy files. Jean-Ralphio sings üé∂statelessüé∂\nKeep it secret, keep it safe For this setup to work, I need some secret values that I won‚Äôt share with anyone. Remember when I decided to go for ansible instead of cloud-init? This is what I attempted to foreshadow. Notably, I need:\nBigQuery credentials S3 credentials I‚Äôm not interested in anyone in the entire world ever getting access to these, other than me. This is what I wanted to use ansible-vault for. So let‚Äôs initialize a secrets file with ansible-vault:\nansible-vault create secrets.yml New Vault password: Confirm New Vault password: After confirming my password, it opens my $EDITOR (meaning emacs, naturally) and I can enter secrets. I chose the password ‚Äúhugabuga‚Äù for this one, and I will populate it with the wrong secret values but the right keys, so you can play with it if you want. You should make a much better password. This one is taken!\nThe contents should look like this:\naws_access_key_id: \"Did you really think\" aws_secret_access_key: \"I would put real credentials in here\" bq_service_account: \"And share the password?\" Once I save the file and close it, ansible-vault encrypts it, and it looks like this:\n$ANSIBLE_VAULT;1.1;AES256 30313539343136313831616265626561646563323064313538346666623032646136666338613137 3536366234316664373331326463613965343132306339370a313539346231656131373637303931 61616630653635343231333138383763316661326233626535666430643930383565346436646662 3737626532656538370a333263343132323832636362633064633536336133363464346363633637 35326439356664326666383963636535313132323536376266623434646631316533653731326461 30643838323265643063343039616537373632663165646463636330626234363766383635656531 35623763643963316362313662663032333961303230333165363232363064626332363335663461 62633634303937623036393562333561666231346366616363323735653531313836333536376362 37326132306535386664616661326131303433316130343136396437653563323264313031323263 63613462333661646235396664306661643839653363343938393034626439316565653530393036 66313063373335316535613131386530616538323036343932633565653138303737383334336431 66396335313534316232 AES256 is pretty strong, so in theory I can share the real file with people I don‚Äôt trust. But there‚Äôs no real reason for me to do that, so out of an abundance of caution, I won‚Äôt.\nThis presents a new challenge, though. vagrant won‚Äôt know how to open this, so now we need to learn how to invoke ansible ourselves. Sigh.\nThe inventory file Since Ansible can provision many hosts all at once, it has a concept of an inventory file. We‚Äôll create a simple one that works with our vagrant setup. Let‚Äôs put this in hosts.ini:\n[vagrant] 127.0.0.1 [vagrant:vars] ansible_ssh_user=admin ansible_become_user=root ansible_ssh_port=2222 üí°A hostgroup can have many hosts in it! Place a new hostname or IP on each line in the block. You can call it anything you‚Äôd like, but you‚Äôll need to use $name:vars to set common variables to those hosts if you need to. With many hosts, running ansible can be slow. If that‚Äôs a problem for you, there are some useful tricks you can use.\nNow we can apply our ansible playbook to the vagrant host group using this command:\n# --ask-vaultpass gives you a prompt # we could also use --vault-password-file if we were too lazy to type hugabuga ansible-playbook -i hosts.ini --limit vagrant --ask-vault-pass initialize.yml Eventually we‚Äôll probably want to add a [prod] section and a [prod:vars] section.\nWriting the kollektivkart role This new role is going to look a little bit like the eugene role, so let‚Äôs create the same directory tree for it:\nmkdir -p roles/kollektivkart/{tasks,meta,defaults,templates,files} Let‚Äôs also reuse the same env_suffix variable from eugene, and modify initialize.yml:\n--- - name: Initialize Ubuntu host hosts: all become: true vars: authorized_keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIE9K1p8B8FRCWJ0Ax4obDu+UsLzGgXDIdTYkCZ8FF54b vars_files: - secrets.yml roles: - name: base-install - name: podman - name: caddy - name: eugene vars: env_suffix: -test - name: kollektivkart vars: env_suffix: -test Notice how we added vars_files! This is the crucial part that makes ansible and ansible-vault work together, otherwise they‚Äôd just wander off in different directions, and we would be very confused and have no secrets.\nWe can reuse the exact same meta/main.yml file:\ncp roles/{eugene,kollektivkart}/meta/main.yml But we will want a different default username, so let‚Äôs put this in roles/kollektivkart/defaults/main.yml:\nuser: kollektivkart Next, I‚Äôll place the bq credentials in the home folder belonging to the kollektivkart user and verify that everything seems to work. This next part goes in roles/kollektivkart/tasks/main.yml:\n--- - name: Set up bq credentials copy: dest: \"/home/{{ user }}/bq_credentials\" mode: \"0600\" owner: \"{{ user }}\" group: \"{{ user }}\" content: \"{{ bq_service_account }}\" We run the long-winded ansible-command again and check that it did what we wanted:\nssh -p2222 kollektivkart@localhost cat bq_credentials And share the password? Right, we can use secrets now. The kollektivkart webapp requires some additional configuration. It uses a bunch of environment variables, so we‚Äôll put them in an environment file that podman can use. Since this contains secrets, we‚Äôll use a template. Let‚Äôs create roles/kollektivkart/templates/kollektivkart.conf.j2:\nSIMPLE_ANALYTICS=true AWS_REGION=hel1.your-objectstorage.com DUCKDB_S3_ENDPOINT=hel1.your-objectstorage.com AWS_ACCESS_KEY_ID={{ aws_access_key_id }} AWS_SECRET_ACCESS_KEY={{ aws_secret_access_key }} PARQUET_LOCATION=s3://kaaveland-private-bus-eta We‚Äôll need to add a task to roles/kollektivkart/tasks/main.yml to render the template:\n- name: Set up kollektivkart.conf template: dest: \"/home/{{ user }}/kollektivkart.conf\" src: kollektivkart.conf.j2 mode: \"0600\" owner: \"{{ user }}\" group: \"{{ user }}\" We should probably find a way to use podman secrets for this eventually, but for now, an env file will do.\nQuadlet templates and automatically starting up on boot When kollektivkart starts up, it reads a lot of data from S3 into memory. This takes a fairly long time, it‚Äôs nothing like eugene-web at all. It also takes a long time to respond to requests. There‚Äôs no way we can restart this thing without disappearing from the internet for a while. Or is there?\nActually, we could run two containers and restart them one at a time. There‚Äôs probably a way to make caddy discover when one is down. With our current network setup, that means we‚Äôll want to start the same exact setup on two different http ports.\nsystemd makes this really easy. Let‚Äôs add a task to write the quadlet:\n- name: Set up kollektivkart quadlet copy: dest: \"/home/{{ user }}/.config/containers/systemd/kollektivkart@.container\" owner: \"{{ user }}\" group: \"{{ user }}\" mode: \"0600\" content: | [Unit] Description=Kollektivkart API container on port %i After=network.target [Container] Image=ghcr.io/kaaveland/bus-eta:latest PublishPort=127.0.0.1:%i:8000 Entrypoint=/app/.venv/bin/gunicorn Exec=kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir=/app --workers=3 AutoUpdate=registry EnvironmentFile=/home/{{ user }}/kollektivkart.conf [Service] SyslogIdentifier=kollektivkart Restart=on-failure CPUQuota=200% MemoryMax=3G There are two strange new things introduced here:\nThe quadlet name is kollektivkart@.container. This makes the file a quadlet template instead of a regular quadlet. We publish the port to 127.0.0.1:%i. The %i is where we receive the template parameter. We use it for a port, but it can be anything! Be creative. We‚Äôll provision this with the long-winded ansible command, then play around in the shell a little bit:\nssh -p 2222 kollektivkart@localhost kollektivkart@127:~$ systemctl --user daemon-reload kollektivkart@127:~$ systemctl --user start kollektivkart@8000 kollektivkart@127:~$ systemctl --user start kollektivkart@8001 kollektivkart@127:~$ systemctl --user status ‚óè 127.0.0.1 State: running Units: 152 loaded (incl. loaded aliases) Jobs: 0 queued Failed: 0 units Since: Tue 2025-05-13 22:49:06 CEST; 1h 1min ago systemd: 255.4-1ubuntu8.6 CGroup: /user.slice/user-1003.slice/user@1003.service ‚îú‚îÄapp.slice ‚îÇ ‚îî‚îÄapp-kollektivkart.slice ‚îÇ ‚îú‚îÄkollektivkart@8000.service ‚îÇ ‚îÇ ‚îú‚îÄlibpod-payload-fdd0fbf5a81144fdfc6d8383debbd0808cd4a569dadba51e9273280bec5bab8b ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ57618 /app/.venv/bin/python /app/.venv/bin/gunicorn kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir=/app --workers=3 ‚îÇ ‚îÇ ‚îî‚îÄruntime ‚îÇ ‚îÇ ‚îú‚îÄ57612 /usr/bin/pasta --config-net -t 127.0.0.1/8000-8000:8000-8000 --dns-forward 169.254.1.1 -u none -T none -U none --no-map-gw --quiet --netns /run/user/1003/netns/netns-816a4334-4656-6ff0-3\u003e ‚îÇ ‚îÇ ‚îî‚îÄ57616 /usr/bin/conmon --api-version 1 -c fdd0fbf5a81144fdfc6d8383debbd0808cd4a569dadba51e9273280bec5bab8b -u fdd0fbf5a81144fdfc6d8383debbd0808cd4a569dadba51e9273280bec5bab8b -r /usr/bin/crun -\u003e ‚îÇ ‚îî‚îÄkollektivkart@8001.service ‚îÇ ‚îú‚îÄlibpod-payload-67e2568560852fe2f5c1ed0d41893dfd7b58b3ee8c5f8e9fba3956460aa476c4 ‚îÇ ‚îÇ ‚îî‚îÄ57655 /app/.venv/bin/python /app/.venv/bin/gunicorn kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir=/app --workers=3 ‚îÇ ‚îî‚îÄruntime ‚îÇ ‚îú‚îÄ57650 /usr/bin/pasta --config-net -t 127.0.0.1/8001-8001:8000-8000 --dns-forward 169.254.1.1 -u none -T none -U none --no-map-gw --quiet --netns /run/user/1003/netns/netns-af5d94e5-41fc-cc37-3\u003e ‚îÇ ‚îî‚îÄ57653 /usr/bin/conmon --api-version 1 -c 67e2568560852fe2f5c1ed0d41893dfd7b58b3ee8c5f8e9fba3956460aa476c4 -u 67e2568560852fe2f5c1ed0d41893dfd7b58b3ee8c5f8e9fba3956460aa476c4 -r /usr/bin/crun -\u003e This is a very long-winded way of saying that we can very easily start this container in many replicas now, each on different ports. It‚Äôs stuck in a reboot-loop, though:\nkollektivkart@127:~$ journalctl --user | grep restart | tail -n1 May 13 16:53:42 127.0.0.1 systemd[39527]: kollektivkart@8000.service: Scheduled restart job, restart counter is at 42. kollektivkart@127:~$ systemctl --user stop kollektivkart@8000 kollektivkart@127:~$ systemctl --user stop kollektivkart@8001 That‚Äôs because we aren‚Äôt using the right S3 access key for some reason. I can easily fix that, but you can‚Äôt! There are literally secrets between us.\nOne thing that you cannot do with a quadlet template is to systemctl enable it. So how do we make sure it comes up on boot? The best way I‚Äôve found is to make a regular systemd unit that boots the template. We can add this to roles/kollektivkart/tasks/main.yml:\n- name: Set up kollektivkart start on boot copy: dest: \"/home/{{ user }}/.config/systemd/user/kollektivkart-starter.service\" owner: \"{{ user }}\" group: \"{{ user }}\" mode: \"0600\" content: | [Unit] Description=Start Kollektivkart Application Instances (8000 and 8001) After=network.target [Service] Type=oneshot RemainAfterExit=yes ExecStart=/usr/bin/systemctl --user start kollektivkart@8000 ExecStart=/usr/bin/systemctl --user start kollektivkart@8001 [Install] WantedBy=default.target Now we can do our machinectl shenanigans by adding these tasks to roles/kollektivkart/tasks/main.yml:\n- name: Reload systemd command: machinectl shell {{ user }}@ /bin/systemctl --user daemon-reload - name: Enable kollektivkart command: machinectl shell {{ user }}@ /bin/systemctl --user enable kollektivkart-starter After running ansible-playbook one more time, let‚Äôs try booting the machine:\nssh -p2222 admin@localhost sudo reboot 0 ssh -p2222 kollektivkart@localhost systemctl --user status ‚óè 127.0.0.1 State: running Units: 153 loaded (incl. loaded aliases) Jobs: 0 queued Failed: 0 units Since: Tue 2025-05-13 23:21:22 CEST; 3s ago systemd: 255.4-1ubuntu8.6 CGroup: /user.slice/user-1003.slice/user@1003.service ‚îú‚îÄapp.slice ‚îÇ ‚îî‚îÄapp-kollektivkart.slice ‚îÇ ‚îú‚îÄkollektivkart@8000.service ‚îÇ ‚îÇ ‚îú‚îÄlibpod-payload-bb7340e633ea971aa8c57123e2909e40c744d2371ff70a8d2bba4a682244baa9 ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ1871 /app/.venv/bin/python /app/.venv/bin/gunicorn kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir=/app --workers=3 ... Success! But so far, nobody can reach this service without using ssh. We‚Äôll need to configure caddy to proxy to these two instances.\nProxy configuration Let‚Äôs make a template for the configuration in roles/kollektivkart/templates/kollektivkart.caddy.j2:\nkollektivkart{{ env_suffix }}.kaveland.no { encode reverse_proxy { to localhost:8000 localhost:8001 health_uri /ready health_interval 2s health_timeout 1s health_status 200 } log } We‚Äôre setting up caddy to proxy for both instances, relying on it to discover on the /ready HTTP-endpoint when the backend can receive traffic. We‚Äôll let it check every two seconds. If we take down a container, caddy should stop sending requests to it within a couple of seconds. This is a good enough quality of service for my hobby projects. We need to add rendering of the template to roles/kollektivkart/tasks/main.yml:\n- name: Configure reverse proxy template: dest: \"/etc/caddy/proxies.d/kollektivkart.caddy\" src: kollektivkart.caddy.j2 owner: root mode: \"0644\" notify: reload caddy But what about my job? The same image contains a job that needs to run once per night to ingest new public transit data from data.entur.no. Once it‚Äôs crunched the latest data, we need to restart the kollektivkart service. We already decided that losing requests for ~2 seconds at a time is fine. We‚Äôll put a small rolling-restart script on the server. Let‚Äôs start by putting this file in roles/kollektivkart/files/rolling-restart:\n#!/usr/bin/env bash set -euf -o pipefail echo \"Perform rolling-restart of kollektivkart@8000 and kollektivkart@8001\" for port in 8000 8001; do systemctl --user restart kollektivkart@$port for attempt in $(seq 1 20); do sleep 3; if curl -s -o /dev/null -w \"\" -f http://localhost:$port/ready; then break; else echo \"kollektivkart@$port Not up after attempt number $attempt, sleeping\" fi if [ \"$attempt\" -eq 20 ] \u0026\u0026 ! curl -s -o /dev/null -w \"\" -f http://localhost:$port/ready; then echo \"kollektivkart@$port failed to start after 20 attempts\" exit 1 fi done done We could easily generalize this, we‚Äôll do that later if we discover that we need to reuse it. Restarting kollektivkart generally takes about 10 seconds; our script gives up after 60. That should be fine. If the first container does not come back up after a restart, we‚Äôll be at half-capacity. That‚Äôs fine; we have two containers for the redundancy, not the capacity. The script will fail in that case. We‚Äôll have to look into getting some sort of monitoring for that later.\nLet‚Äôs add it to the server by making a task in roles/kollektivkart/tasks/main.yml:\n- name: Set up rolling-restart script copy: dest: \"/home/{{ user }}/rolling-restart\" src: \"rolling-restart\" owner: \"{{ user }}\" group: \"{{ user }}\" mode: \"0744\" Now we can configure a quadlet for our job in roles/kollektivkart/tasks/main.yml:\n- name: Set up kollektivkart etl quadlet copy: dest: \"/home/{{ user }}/.config/containers/systemd/kollektivkart-etl.container\" owner: kollektivkart mode: \"0600\" content: | [Unit] Description=Kollektivkart ETL container After=network.target [Container] Image=ghcr.io/kaaveland/bus-eta:latest Volume=/home/{{ user }}/bq_credentials:/bq_credentials:ro Environment=GOOGLE_APPLICATION_CREDENTIALS=/bq_credentials Entrypoint=/app/.venv/bin/python Exec=-m kollektivkart.etl ${PARQUET_LOCATION} --memory-limit-gb 3 --max-cpus 2 AutoUpdate=registry EnvironmentFile=/home/{{ user }}/kollektivkart.conf [Service] Type=oneshot # This critical line lets systemd find the ${PARQUET_LOCATION} env var from this file # so we can pass it on the command line in the block above EnvironmentFile=/home/{{ user }}/kollektivkart.conf SyslogIdentifier=kollektivkart-etl CPUQuota=200% MemoryMax=4G WorkingDirectory=/home/{{ user }} ExecStartPost=/home/{{ user }}/rolling-restart [Install] WantedBy=default.target It may look strange that we‚Äôre passing --memory-limit-gb and --max-cpus, but the reason for that is to inform DuckDB about how much capacity it has. Otherwise, it might detect all the CPU cores and try to use more resources than we‚Äôve allowed for it. CPUQuota=200% doesn‚Äôt prevent it from seeing how many cores the machine has, it is only a scheduling guarantee. It probably wouldn‚Äôt hurt to let DuckDB use 33% on each of our six cores, but it seems friendlier to let it use two whole ones. ü§ó\nThe job needs an extra environment variable compared to the webapp, the webapp never accesses BigQuery directly. The [Service] section has ExecStartPost. This is a somewhat strange name-choice, I think, for a command that is to be run once the script is done. So, this systemd unit will run a container once, then do the rolling restart. But nothing actually starts this container, so we have to take care of that too. We can use a systemd timer for this, let‚Äôs write it:\n- name: Set up kollektivkart etl nightly timer copy: dest: \"/home/{{ user }}/.config/systemd/user/kollektivkart-etl.timer\" owner: \"{{ user }}\" group: \"{{ user }}\" mode: \"0600\" content: | [Unit] Description=Run Kollektivkart ETL nightly RefuseManualStart=false RefuseManualStop=false [Timer] OnCalendar=*-*-* 04:00:00 RandomizedDelaySec=30m Persistent=true Unit=kollektivkart-etl.service [Install] WantedBy=timers.target - name: Enable kollektivkart-etl command: machinectl shell {{ user }}@ /bin/systemctl --user enable kollektivkart-etl.timer - name: Start kollektivkart-etl timer command: machinectl shell {{ user }}@ /bin/systemctl --user start kollektivkart-etl.timer This is a lot like a cronjob. We‚Äôve set it to go off at 04:00, with a randomized delay of up to 30 minutes. It‚Äôs also set to persistent, which means if the machine is off from 04:00‚Äì04:30, it‚Äôll decide to run this job once it boots. Let‚Äôs check if this worked:\nssh -p2222 kollektivkart@localhost systemctl --user status kollektivkart-etl.timer ‚óè kollektivkart-etl.timer - Run Kollektivkart ETL nightly Loaded: loaded (/home/kollektivkart/.config/systemd/user/kollektivkart-etl.timer; enabled; preset: enabled) Active: active (waiting) since Tue 2025-05-13 23:16:21 CEST; 2s ago Trigger: Wed 2025-05-14 04:07:08 CEST; 9h left Triggers: ‚óè kollektivkart-etl.service That looks good! The job itself won‚Äôt actually work without the correct BigQuery or S3 credentials, but everything‚Äôs configured now.\nSetting up a test server We actually have everything we need to make this come alive on the internet now. If you read this far, you have my undying respect. Maybe you learned something?\nI‚Äôm going to quickly clickops a server in hetzner and point api-test.kaveland.no and kollektivkart-test.kaveland.no to it, then see if everything comes up. I name my personal servers after whisky distilleries. This one is going to be called dalwhinnie.\nDNS I‚Äôm setting up A and AAAA records for dalwhinnie.kaveland.no, and putting this in my hosts.ini:\n[test] dalwhinnie.kaveland.no [test:vars] ansible_ssh_user=root # will later need to change to admin after first time provisioning ansible_become_user=root I‚Äôll make CNAME records for api-test.kaveland.no and kollektivkart-test.kaveland.no pointing to dalwhinnie.kaveland.no.\nI‚Äôm removing the secrets.yml from the playbook and running this:\nansible-playbook -i hosts.ini --limit test --ask-vault-pass initialize.yml \\ -e @~/code/infra/secrets.yml # reads secrets from the real vault It applies OK to the brand-new server. A few moments later, once caddy has gotten certificates:\ncurl -I https://api-test.kaveland.no/app/eugene/random.sql HTTP/2 200 alt-svc: h3=\":443\"; ma=2592000 server: Caddy date: Tue, 13 May 2025 17:22:50 GMT curl -I https://kollektivkart-test.kaveland.no/ HTTP/2 200 alt-svc: h3=\":443\"; ma=2592000 content-type: text/html; charset=utf-8 date: Tue, 13 May 2025 17:23:38 GMT server: Caddy server: gunicorn content-length: 4161 And we‚Äôre in business! Caddy works as advertised. It took a minute or so to get certificates. If you notice time-traveling in the timestamps here, don‚Äôt worry. I‚Äôm not making a paradox. I‚Äôve just rerun some commands above in the late evening. Didn‚Äôt mean to spook you. üëª\nI did a quick reboot here and verified that everything came up, and the kollektivkart-etl job started automatically. The rolling-restart script works well enough for my purposes (I observed about 2 seconds of downtime, as expected). I deleted the server afterward. I can trivially make a new one.\nStatic assets I don‚Äôt host static assets from the server, I rely on bunny.net for that. Read more here if you‚Äôd like. This costs around $1 a month, for much better worldwide performance than I could ever achieve on a single server. Totally worth it. Bunny also has a container hosting service that would be very suitable for eugene-web.\nMonitoring The way I‚Äôve set this up, I must expect reboots. At some point, my entire infrastructure will be down. Since I‚Äôm planning on running only a single server and could move it around a bit, my best option here is to use something external.\nI‚Äôve set up a statuspage with phare.io. At my level of usage, this is a free service. It pings three different URLs I run every few minutes, and it will email me if they stay down for a while. This was super easy to set up, and works very well. I inadvertently tested this by disabling DNSSEC on my domain before getting rid of the DS record the other day ü´† Going to write about everything I learned about DNSSEC from that in the future!\nPhare works fine for DNSSEC-related outages. Take my word for it. You can find a more harmless way to test it!\nFor things like my ETL-job, I‚Äôll make a URL on my page that returns some status code other than 200 if the data is stale, and phare.io will notify me if the job has had some issue. I don‚Äôt have a plan right now for detecting that a rolling restart failed, but something will come to me.\nFor the moment, I do not have anything better than /var/log/syslog and journalctl for viewing logs, and sar for viewing server load and resource consumption over time. That will do for a while, I think, it‚Äôs not like I get a lot of traffic.\nTechnical debt we could fix The big one here feels like the proxy setup and the port numbers. Here‚Äôs what we did wrong:\nPutting the proxy hostname in at the app level. Putting the port numbers at the app level. The hostname and the port numbers are what caddy needs to know about. I think we should probably have made an proxy-endpoint role that could connect the two, something like this üßê\nroles: - name: base-install - name: eugene vars: ports: - 3000 - name: proxy-endpoint vars: hostname: \"api{{ env_suffix }}.kaveland.no\" routes: - path: /eugene ports: - 3000 We could always consider that later, but it feels like a lot of trouble right now. For such a small setup, the best solution is probably to just use one central Caddyfile.\nWe should also consider having more playbooks. initialize.yml should be more bare-bones than it is, possibly do only the security-related things. That way, we could let it override the initial username to log in with, since we disable ssh access for the root user later.\nWe‚Äôll definitely find a reason to generalize the rolling-restart script at some point, or perhaps replace it with something that uses the caddy management api to drain the backend before we restart it.\nWe‚Äôve put image tags directly in the quadlets. That‚Äôs not a great idea. It means we need to edit complex and annoying files to roll back, possibly while under stress. We should probably put the tag somewhere it‚Äôs easier to edit. Quadlet files can reference environment files, so this is easily doable.\nArchitectural weakness I currently have my S3 bucket and my server in the same region in Hetzner. This is only okay because I can recreate the data perfectly from an external source. Normally I would advocate very strongly for having data be replicated to other regions. Data centers can burn down.\nOn the whole, there‚Äôs not a lot of redundancy. If the machine is down, my things are down. The fact that making a new server is fast makes this point less painful. It is straightforward to scale to a bigger machine. It‚Äôs possible to scale out to more machines too, but that requires tinkering more with DNS, TLS and load-balancing.\nThere‚Äôs no persistent database in this setup. I believe I have the chops to set up a reasonably stable postgres installation with pgBackRest that ships backups to elsewhere eventually. But hosting a database is no joke, this is something I would advise anyone to consider buying as a service. It might seem expensive, but it‚Äôs not.\nFor hobby purposes, I think all of these are non-issues, but I would advise spending more time on a setup like this if I were running a business. Or perhaps accept the cost of buying managed services; there are many that are worth paying for.\nWhy go through all this trouble? I could say that it was about the journey, not the destination. That wouldn‚Äôt be wrong, exactly. I enjoy tinkering with Linux servers, and I feel like I learned a lot doing this exercise. As a developer, I find myself building on top of leaky abstractions all the time. It is good to know a thing or two about what‚Äôs underneath whatever abstraction I deploy my software on. This is a fairly lean setup for a machine. I like that.\nThe liberty to host my stuff wherever I want is important to me right now. Being able to put everything on a Linux server with a simple ansible-playbook makes me very flexible. This kind of deployment is possible at almost any provider, and means I get to test all the european providers I‚Äôm interested in.\nThere‚Äôs a fairly large initial time investment, but that‚Äôs mostly done now. Setting up a new container is going to be rapid and simple in the future, now that all the pieces are in place.\nI‚Äôm currently running on bare metal, so the price/performance ratio is hard to beat. I have 6 physical CPU cores, 64GB of RAM and 500GB of NVMe SSD at a very reasonable ‚Ç¨46.63/month, including VAT. For comparison, this costs significantly less than a 2VCPU 8GB RAM D2_V3 in Azure, and I have no risk of noisy neighbors impeding on my VCPU time. I have no excuses for writing adequately efficient SQL anymore. I must drop all indexes immediately.\nIf I decide that I have no need for bare metal, I‚Äôll go back to 4 VCPU, 8GB RAM and 80GB NVMe SSD at ‚Ç¨7.8/month. This is enough to run what I have right now, I just bought something bigger to force me to have bigger ambitions.\nIf it turns out that there are a lot of issues with running like this, I can find some managed k8s or container solution instead, and I wouldn‚Äôt have lost anything but time. But the time already paid for itself with increased knowledge and the entertainment of learning new things üéìü•≥\nWhat did we learn? Here‚Äôs a short list:\npodman and systemd integrate very nicely now. quadlet templates are incredibly powerful and elegant! Just the right level of abstraction for this kind of project. Caddy makes it very trivial to do TLS with letsencrypt. unattended-upgrades can take care of patching. k8s and hosting solutions like fly.io or heroku do a lot of heavy lifting for you. Heavy lifting can be healthy, though! It‚Äôs good to understand some of the problems they solve. There‚Äôs a reason why people are paying good üí∏ to have all this stuff be someone else‚Äôs problem. Stateless backends are straightforward and pleasant to self-host üéâ Ansible is still alive and kicking, and I even remember some of it. We have barely scratched the surface of what it can do. It‚Äôs powerful software. I think it has a Hetzner module, and DNS-integration with bunny.net, so it could probably automate the last manual steps too. Ansible and Vagrant are a very nice combination for locally developing server configuration. If you‚Äôd like to self-host on your own server, but this setup looks intimidating and complex, I totally get that. You may want to check out options like coolify or dokploy.\nWhere to go from here? If you want to play and tinker with this, all the code is available. I made some minor modifications to make it more convenient to get started, but if you read all the way here, you‚Äôll have no trouble finding your way. Proud of you! Give yourself a pat on the back on my behalf.\nThere‚Äôs a list of things to try in the section about technical debt. Here are some more ideas:\nMaybe try running Caddy in a container? Check if we can use socket activation! This seems like an almost magic way to pass sockets from the host to the container. Set up a podman network! Make an app and deploy it in Hetzner, then move it to upcloud or scaleway. You can go anywhere now! If you‚Äôd like to refer back to an earlier installment, I placed links here for your convenience:\nThe first post covers local development of linux server configuration and essentials. The second post covers installation of podman and caddy. It concludes by deploying a very simple stateless webapp in a container. Writing this piece took a while. If you read all the way to the end, it would mean the world to me if you‚Äôd let me know what you think and perhaps share it with someone. You can find some contact information on my personal page. If there‚Äôs any interest, I might do more projects with a scope like this in the future.\n","wordCount":"4516","inLanguage":"en","datePublished":"2025-05-14T18:00:00Z","dateModified":"2025-05-14T18:00:00Z","author":{"@type":"Person","name":"Robin K√•veland"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kaveland.no/posts/2025-05-14-fire-and-forget-linux-p3/"},"publisher":{"@type":"Organization","name":"Robin's blog","logo":{"@type":"ImageObject","url":"https://kaveland.no/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kaveland.no/ accesskey=h title="Robin's blog (Alt + H)">Robin's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kaveland.no/about/ title=about><span>about</span></a></li><li><a href=https://kaveland.no/projects/ title=projects><span>projects</span></a></li><li><a href=https://kaveland.no/eugene/ title=eugene><span>eugene</span></a></li><li><a href=https://kaveland.no/thumper/ title=thumper><span>thumper</span></a></li><li><a href=https://kaveland.no/tags/ title=tags><span>tags</span></a></li><li><a href=https://kaveland.no/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kaveland.no/>Home</a>&nbsp;¬ª&nbsp;<a href=https://kaveland.no/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">No-ops linux part 3: It puts the data in the pond. Nightly.</h1><div class=post-meta><span title='2025-05-14 18:00:00 +0000 UTC'>May 14, 2025</span>&nbsp;¬∑&nbsp;22 min&nbsp;¬∑&nbsp;4516 words&nbsp;¬∑&nbsp;Robin K√•veland&nbsp;|&nbsp;<a href=https://github.com/kaaveland/kaaveland.github.io/content/posts/2025-05-14-fire-and-forget-linux-p3.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introducing-the-kollektivkart-data-pond>Introducing the kollektivkart ‚ú®data pond‚ú®</a><ul><li><a href=#keep-it-secret-keep-it-safe>Keep it secret, keep it safe</a></li><li><a href=#the-inventory-file>The inventory file</a></li><li><a href=#writing-the-kollektivkart-role>Writing the kollektivkart role</a></li></ul></li><li><a href=#setting-up-a-test-server>Setting up a test server</a><ul><li><a href=#dns>DNS</a></li></ul></li><li><a href=#static-assets>Static assets</a></li><li><a href=#monitoring>Monitoring</a></li><li><a href=#technical-debt-we-could-fix>Technical debt we could fix</a></li><li><a href=#architectural-weakness>Architectural weakness</a></li><li><a href=#why-go-through-all-this-trouble>Why go through all this trouble?</a></li><li><a href=#what-did-we-learn>What did we learn?</a></li><li><a href=#where-to-go-from-here>Where to go from here?</a></li></ul></nav></div></details></div><div class=post-content><p>This post is part of the series on no-ops linux deployment. The <a href=/posts/2025-05-13-fire-and-forget-linux-p1>first post</a> covered local development of linux server configuration and essential configuration. The <a href=/posts/2025-05-14-fire-and-forget-linux-p2>previous installment</a> covers a janky podman installation and configures a reverse proxy to send traffic to a simple container deployment. This is the <a href=/posts/2025-05-14-fire-and-forget-linux-p3>final post</a>. It covers a more challenging deployment with jobs and rolling restarts, and discusses the strengths and weaknesses of this approach to hosting.</p><p>After the previous post, we know how to deploy a container that requires absolutely no configuration and restarts almost instantly. Most of the applications I work on in my daytime job aren&rsquo;t like that. Let&rsquo;s take a look at a more complex example.</p><h2 id=introducing-the-kollektivkart-data-pond>Introducing the kollektivkart ‚ú®data pond‚ú®<a hidden class=anchor aria-hidden=true href=#introducing-the-kollektivkart-data-pond>#</a></h2><p><a href=https://kollektivkart.arktekk.no>kollektivkart</a> pulls data from Google BigQuery to S3-compatible storage, runs some <a href=https://duckdb.org>DuckDB</a> queries on it and shows it in a map (somewhat simplified). The data set it pulls from is open data, and documented at <a href=https://data.entur.no>data.entur.no</a>. The <a href=https://github.com/kaaveland/bus-eta>source code</a> is freely available, so you can steal it if you wish.</p><p>This service <em>could easily</em> use a local disk. It pulls down around 20GB of data from BigQuery as partitioned parquet datasets. After crunching everything I find interesting, it occupies around 40GB of space, including around 700 million rows with 21 columns of raw data, 400 million rows with 18 columns of refined data and 6 million rows of aggregated data that can be visualized. This will work fine on even a cheap cloud virtual machine. What a time to be alive.</p><p>It is incredibly nice to make the server stateless, to the degree that I can. This ensures that I can quickly and easily replace the machine with another one. So, that&rsquo;s what I&rsquo;ll do. The jobs and the webapp can both read/write from <code>s3://</code> paths or from a local disk. We&rsquo;ll do it on hard-mode and configure it to work with S3, that way, we can move the app to new servers without having to copy files. Jean-Ralphio sings üé∂statelessüé∂</p><h3 id=keep-it-secret-keep-it-safe>Keep it secret, keep it safe<a hidden class=anchor aria-hidden=true href=#keep-it-secret-keep-it-safe>#</a></h3><p>For this setup to work, I need some secret values that I won&rsquo;t share with anyone. Remember when I decided to go for ansible instead of cloud-init? This is what I attempted to foreshadow. Notably, I need:</p><ul><li>BigQuery credentials</li><li>S3 credentials</li></ul><p>I&rsquo;m not interested in anyone in the entire world ever getting access to these, other than me. This is what I wanted to use <code>ansible-vault</code> for. So let&rsquo;s initialize a secrets file with ansible-vault:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ansible-vault create secrets.yml
</span></span><span class=line><span class=cl>New Vault password:
</span></span><span class=line><span class=cl>Confirm New Vault password:
</span></span></code></pre></div><p>After confirming my password, it opens my <code>$EDITOR</code> (meaning emacs, naturally) and I can enter secrets. I chose the password &ldquo;hugabuga&rdquo; for this one, and I will populate it with the wrong secret values but the right keys, so you can play with it if you want. You should make a much better password. This one is taken!</p><p>The contents should look like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>aws_access_key_id</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;Did you really think&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>aws_secret_access_key</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;I would put real credentials in here&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>bq_service_account</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;And share the password?&#34;</span><span class=w>
</span></span></span></code></pre></div><p>Once I save the file and close it, ansible-vault encrypts it, and it looks like this:</p><pre tabindex=0><code>$ANSIBLE_VAULT;1.1;AES256
30313539343136313831616265626561646563323064313538346666623032646136666338613137
3536366234316664373331326463613965343132306339370a313539346231656131373637303931
61616630653635343231333138383763316661326233626535666430643930383565346436646662
3737626532656538370a333263343132323832636362633064633536336133363464346363633637
35326439356664326666383963636535313132323536376266623434646631316533653731326461
30643838323265643063343039616537373632663165646463636330626234363766383635656531
35623763643963316362313662663032333961303230333165363232363064626332363335663461
62633634303937623036393562333561666231346366616363323735653531313836333536376362
37326132306535386664616661326131303433316130343136396437653563323264313031323263
63613462333661646235396664306661643839653363343938393034626439316565653530393036
66313063373335316535613131386530616538323036343932633565653138303737383334336431
66396335313534316232
</code></pre><p>AES256 is pretty strong, so in theory I can share the real file with people I don&rsquo;t trust. But there&rsquo;s no real reason for me to do that, so out of an abundance of caution, I won&rsquo;t.</p><p>This presents a new challenge, though. <code>vagrant</code> won&rsquo;t know how to open this, so now we need to learn how to invoke ansible ourselves. Sigh.</p><h3 id=the-inventory-file>The inventory file<a hidden class=anchor aria-hidden=true href=#the-inventory-file>#</a></h3><p>Since Ansible can provision many hosts all at once, it has a concept of an inventory file. We&rsquo;ll create a simple one that works with our vagrant setup. Let&rsquo;s put this in <code>hosts.ini</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ini data-lang=ini><span class=line><span class=cl><span class=k>[vagrant]</span>
</span></span><span class=line><span class=cl><span class=na>127.0.0.1</span>
</span></span><span class=line><span class=cl><span class=k>[vagrant:vars]</span>
</span></span><span class=line><span class=cl><span class=na>ansible_ssh_user</span><span class=o>=</span><span class=s>admin</span>
</span></span><span class=line><span class=cl><span class=na>ansible_become_user</span><span class=o>=</span><span class=s>root</span>
</span></span><span class=line><span class=cl><span class=na>ansible_ssh_port</span><span class=o>=</span><span class=s>2222</span>
</span></span></code></pre></div><blockquote><p>üí°A hostgroup can have many hosts in it! Place a new hostname or IP on each line in the block. You can call it anything you&rsquo;d like, but you&rsquo;ll need to use <code>$name:vars</code> to set common variables to those hosts if you need to. With many hosts, running ansible can be slow. If that&rsquo;s a problem for you, there are some useful <a href=https://www.redhat.com/en/blog/faster-ansible-playbook-execution>tricks you can use</a>.</p></blockquote><p>Now we can apply our ansible playbook to the vagrant <em>host group</em> using this command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># --ask-vaultpass gives you a prompt</span>
</span></span><span class=line><span class=cl><span class=c1># we could also use --vault-password-file if we were too lazy to type hugabuga </span>
</span></span><span class=line><span class=cl>ansible-playbook -i hosts.ini --limit vagrant --ask-vault-pass initialize.yml 
</span></span></code></pre></div><p>Eventually we&rsquo;ll probably want to add a <code>[prod]</code> section and a <code>[prod:vars]</code> section.</p><h3 id=writing-the-kollektivkart-role>Writing the kollektivkart role<a hidden class=anchor aria-hidden=true href=#writing-the-kollektivkart-role>#</a></h3><p>This new role is going to look a little bit like the eugene role, so let&rsquo;s create the same directory tree for it:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mkdir -p roles/kollektivkart/<span class=o>{</span>tasks,meta,defaults,templates,files<span class=o>}</span>
</span></span></code></pre></div><p>Let&rsquo;s also reuse the same <code>env_suffix</code> variable from <code>eugene</code>, and modify <code>initialize.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Initialize Ubuntu host</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hosts</span><span class=p>:</span><span class=w> </span><span class=l>all</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>become</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>vars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>authorized_keys</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIE9K1p8B8FRCWJ0Ax4obDu+UsLzGgXDIdTYkCZ8FF54b</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>vars_files</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>secrets.yml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>roles</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>base-install</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>podman</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>caddy</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>eugene</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>vars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>env_suffix</span><span class=p>:</span><span class=w> </span>-<span class=l>test</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>kollektivkart</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>vars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>env_suffix</span><span class=p>:</span><span class=w> </span>-<span class=l>test</span><span class=w>
</span></span></span></code></pre></div><p>Notice how we added <code>vars_files</code>! This is the crucial part that makes ansible and ansible-vault work together, otherwise they&rsquo;d just wander off in different directions, and we would be very confused and have no secrets.</p><p>We can reuse the exact same <code>meta/main.yml</code> file:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cp roles/<span class=o>{</span>eugene,kollektivkart<span class=o>}</span>/meta/main.yml
</span></span></code></pre></div><p>But we will want a different default username, so let&rsquo;s put this in <code>roles/kollektivkart/defaults/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>user</span><span class=p>:</span><span class=w> </span><span class=l>kollektivkart</span><span class=w>
</span></span></span></code></pre></div><p>Next, I&rsquo;ll place the bq credentials in the home folder belonging to the kollektivkart user and verify that everything seems to work. This next part goes in <code>roles/kollektivkart/tasks/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up bq credentials</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>copy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/bq_credentials&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0600&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>group</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>content</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ bq_service_account }}&#34;</span><span class=w>
</span></span></span></code></pre></div><p>We run the long-winded ansible-command again and check that it did what we wanted:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ssh -p2222 kollektivkart@localhost cat bq_credentials
</span></span><span class=line><span class=cl>And share the password?
</span></span></code></pre></div><p>Right, we can use secrets now. The kollektivkart webapp requires some additional configuration. It uses a bunch of environment variables, so we&rsquo;ll put them in an environment file that podman can use. Since this contains secrets, we&rsquo;ll use a template. Let&rsquo;s create <code>roles/kollektivkart/templates/kollektivkart.conf.j2</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nv>SIMPLE_ANALYTICS</span><span class=o>=</span><span class=nb>true</span>
</span></span><span class=line><span class=cl><span class=nv>AWS_REGION</span><span class=o>=</span>hel1.your-objectstorage.com
</span></span><span class=line><span class=cl><span class=nv>DUCKDB_S3_ENDPOINT</span><span class=o>=</span>hel1.your-objectstorage.com
</span></span><span class=line><span class=cl><span class=nv>AWS_ACCESS_KEY_ID</span><span class=o>={{</span> aws_access_key_id <span class=o>}}</span>
</span></span><span class=line><span class=cl><span class=nv>AWS_SECRET_ACCESS_KEY</span><span class=o>={{</span> aws_secret_access_key <span class=o>}}</span>
</span></span><span class=line><span class=cl><span class=nv>PARQUET_LOCATION</span><span class=o>=</span>s3://kaaveland-private-bus-eta
</span></span></code></pre></div><p>We&rsquo;ll need to add a task to <code>roles/kollektivkart/tasks/main.yml</code> to render the template:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up kollektivkart.conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/kollektivkart.conf&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>src</span><span class=p>:</span><span class=w> </span><span class=l>kollektivkart.conf.j2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0600&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>group</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span></code></pre></div><p>We should probably find a way to use <a href=https://docs.podman.io/en/latest/markdown/podman-secret-create.1.html>podman secrets</a> for this eventually, but for now, an env file will do.</p><h4 id=quadlet-templates-and-automatically-starting-up-on-boot>Quadlet templates and automatically starting up on boot<a hidden class=anchor aria-hidden=true href=#quadlet-templates-and-automatically-starting-up-on-boot>#</a></h4><p>When kollektivkart starts up, it reads a lot of data from S3 into memory. This takes a fairly long time, it&rsquo;s nothing like <code>eugene-web</code> at all. It also takes a long time to respond to requests. There&rsquo;s no way we can restart this thing without disappearing from the internet for a while. Or is there?</p><p>Actually, we could run two containers and restart them one at a time. There&rsquo;s probably a way to make caddy discover when one is down. With our current network setup, that means we&rsquo;ll want to start the same exact setup on two different http ports.</p><p><code>systemd</code> makes this really easy. Let&rsquo;s add a task to write the quadlet:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up kollektivkart quadlet</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>copy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/.config/containers/systemd/kollektivkart@.container&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>group</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0600&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>content</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Unit]
</span></span></span><span class=line><span class=cl><span class=sd>      Description=Kollektivkart API container on port %i
</span></span></span><span class=line><span class=cl><span class=sd>      After=network.target
</span></span></span><span class=line><span class=cl><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Container]
</span></span></span><span class=line><span class=cl><span class=sd>      Image=ghcr.io/kaaveland/bus-eta:latest
</span></span></span><span class=line><span class=cl><span class=sd>      PublishPort=127.0.0.1:%i:8000
</span></span></span><span class=line><span class=cl><span class=sd>      Entrypoint=/app/.venv/bin/gunicorn
</span></span></span><span class=line><span class=cl><span class=sd>      Exec=kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir=/app --workers=3
</span></span></span><span class=line><span class=cl><span class=sd>      AutoUpdate=registry
</span></span></span><span class=line><span class=cl><span class=sd>      EnvironmentFile=/home/{{ user }}/kollektivkart.conf
</span></span></span><span class=line><span class=cl><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Service]
</span></span></span><span class=line><span class=cl><span class=sd>      SyslogIdentifier=kollektivkart
</span></span></span><span class=line><span class=cl><span class=sd>      Restart=on-failure
</span></span></span><span class=line><span class=cl><span class=sd>      CPUQuota=200%
</span></span></span><span class=line><span class=cl><span class=sd>      MemoryMax=3G</span><span class=w>
</span></span></span></code></pre></div><p>There are two strange new things introduced here:</p><ol><li>The quadlet name is <code>kollektivkart@.container</code>. This makes the file a quadlet template instead of a regular quadlet.</li><li>We publish the port to <code>127.0.0.1:%i</code>. The <code>%i</code> is where we receive the template parameter. We use it for a port, but it can be anything! Be creative.</li></ol><p>We&rsquo;ll provision this with the long-winded ansible command, then play around in the shell a little bit:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ssh -p <span class=m>2222</span> kollektivkart@localhost
</span></span><span class=line><span class=cl>kollektivkart@127:~$ systemctl --user daemon-reload
</span></span><span class=line><span class=cl>kollektivkart@127:~$ systemctl --user start kollektivkart@8000
</span></span><span class=line><span class=cl>kollektivkart@127:~$ systemctl --user start kollektivkart@8001
</span></span><span class=line><span class=cl>kollektivkart@127:~$ systemctl --user status
</span></span><span class=line><span class=cl>‚óè 127.0.0.1
</span></span><span class=line><span class=cl>    State: running
</span></span><span class=line><span class=cl>    Units: <span class=m>152</span> loaded <span class=o>(</span>incl. loaded aliases<span class=o>)</span>
</span></span><span class=line><span class=cl>     Jobs: <span class=m>0</span> queued
</span></span><span class=line><span class=cl>   Failed: <span class=m>0</span> units
</span></span><span class=line><span class=cl>    Since: Tue 2025-05-13 22:49:06 CEST<span class=p>;</span> 1h 1min ago
</span></span><span class=line><span class=cl>  systemd: 255.4-1ubuntu8.6
</span></span><span class=line><span class=cl>   CGroup: /user.slice/user-1003.slice/user@1003.service
</span></span><span class=line><span class=cl>           ‚îú‚îÄapp.slice
</span></span><span class=line><span class=cl>           ‚îÇ ‚îî‚îÄapp-kollektivkart.slice
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îú‚îÄkollektivkart@8000.service
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ ‚îú‚îÄlibpod-payload-fdd0fbf5a81144fdfc6d8383debbd0808cd4a569dadba51e9273280bec5bab8b
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ ‚îÇ ‚îî‚îÄ57618 /app/.venv/bin/python /app/.venv/bin/gunicorn kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir<span class=o>=</span>/app --workers<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ ‚îî‚îÄruntime
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ   ‚îú‚îÄ57612 /usr/bin/pasta --config-net -t 127.0.0.1/8000-8000:8000-8000 --dns-forward 169.254.1.1 -u none -T none -U none --no-map-gw --quiet --netns /run/user/1003/netns/netns-816a4334-4656-6ff0-3&gt;
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ   ‚îî‚îÄ57616 /usr/bin/conmon --api-version <span class=m>1</span> -c fdd0fbf5a81144fdfc6d8383debbd0808cd4a569dadba51e9273280bec5bab8b -u fdd0fbf5a81144fdfc6d8383debbd0808cd4a569dadba51e9273280bec5bab8b -r /usr/bin/crun -&gt;
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îî‚îÄkollektivkart@8001.service
</span></span><span class=line><span class=cl>           ‚îÇ     ‚îú‚îÄlibpod-payload-67e2568560852fe2f5c1ed0d41893dfd7b58b3ee8c5f8e9fba3956460aa476c4
</span></span><span class=line><span class=cl>           ‚îÇ     ‚îÇ ‚îî‚îÄ57655 /app/.venv/bin/python /app/.venv/bin/gunicorn kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir<span class=o>=</span>/app --workers<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>           ‚îÇ     ‚îî‚îÄruntime
</span></span><span class=line><span class=cl>           ‚îÇ       ‚îú‚îÄ57650 /usr/bin/pasta --config-net -t 127.0.0.1/8001-8001:8000-8000 --dns-forward 169.254.1.1 -u none -T none -U none --no-map-gw --quiet --netns /run/user/1003/netns/netns-af5d94e5-41fc-cc37-3&gt;
</span></span><span class=line><span class=cl>           ‚îÇ       ‚îî‚îÄ57653 /usr/bin/conmon --api-version <span class=m>1</span> -c 67e2568560852fe2f5c1ed0d41893dfd7b58b3ee8c5f8e9fba3956460aa476c4 -u 67e2568560852fe2f5c1ed0d41893dfd7b58b3ee8c5f8e9fba3956460aa476c4 -r /usr/bin/crun -&gt;
</span></span></code></pre></div><p>This is a very long-winded way of saying that we can very easily start this container in many replicas now, each on different ports. It&rsquo;s stuck in a reboot-loop, though:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kollektivkart@127:~$ journalctl --user <span class=p>|</span> grep restart <span class=p>|</span> tail -n1
</span></span><span class=line><span class=cl>May <span class=m>13</span> 16:53:42 127.0.0.1 systemd<span class=o>[</span>39527<span class=o>]</span>: kollektivkart@8000.service: Scheduled restart job, restart counter is at 42.
</span></span><span class=line><span class=cl>kollektivkart@127:~$ systemctl --user stop kollektivkart@8000
</span></span><span class=line><span class=cl>kollektivkart@127:~$ systemctl --user stop kollektivkart@8001
</span></span></code></pre></div><p>That&rsquo;s because we aren&rsquo;t using the right S3 access key for some reason. I can easily fix that, but you can&rsquo;t! There are literally secrets between us.</p><p>One thing that you cannot do with a quadlet template is to <code>systemctl enable</code> it. So how do we make sure it comes up on boot? The best way I&rsquo;ve found is to make a regular systemd unit that boots the template. We can add this to <code>roles/kollektivkart/tasks/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up kollektivkart start on boot</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>copy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/.config/systemd/user/kollektivkart-starter.service&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>group</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0600&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>content</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Unit]
</span></span></span><span class=line><span class=cl><span class=sd>      Description=Start Kollektivkart Application Instances (8000 and 8001)
</span></span></span><span class=line><span class=cl><span class=sd>      After=network.target
</span></span></span><span class=line><span class=cl><span class=sd>      [Service]
</span></span></span><span class=line><span class=cl><span class=sd>      Type=oneshot
</span></span></span><span class=line><span class=cl><span class=sd>      RemainAfterExit=yes
</span></span></span><span class=line><span class=cl><span class=sd>      ExecStart=/usr/bin/systemctl --user start kollektivkart@8000
</span></span></span><span class=line><span class=cl><span class=sd>      ExecStart=/usr/bin/systemctl --user start kollektivkart@8001
</span></span></span><span class=line><span class=cl><span class=sd>      [Install]
</span></span></span><span class=line><span class=cl><span class=sd>      WantedBy=default.target</span><span class=w>
</span></span></span></code></pre></div><p>Now we can do our <code>machinectl</code> shenanigans by adding these tasks to <code>roles/kollektivkart/tasks/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Reload systemd</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>machinectl shell {{ user }}@ /bin/systemctl --user daemon-reload</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Enable kollektivkart</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>machinectl shell {{ user }}@ /bin/systemctl --user enable kollektivkart-starter</span><span class=w>
</span></span></span></code></pre></div><p>After running <code>ansible-playbook</code> one more time, let&rsquo;s try booting the machine:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ssh -p2222 admin@localhost sudo reboot <span class=m>0</span>
</span></span><span class=line><span class=cl>ssh -p2222 kollektivkart@localhost systemctl --user status
</span></span><span class=line><span class=cl>‚óè 127.0.0.1
</span></span><span class=line><span class=cl>    State: running
</span></span><span class=line><span class=cl>    Units: <span class=m>153</span> loaded <span class=o>(</span>incl. loaded aliases<span class=o>)</span>
</span></span><span class=line><span class=cl>     Jobs: <span class=m>0</span> queued
</span></span><span class=line><span class=cl>   Failed: <span class=m>0</span> units
</span></span><span class=line><span class=cl>    Since: Tue 2025-05-13 23:21:22 CEST<span class=p>;</span> 3s ago
</span></span><span class=line><span class=cl>  systemd: 255.4-1ubuntu8.6
</span></span><span class=line><span class=cl>   CGroup: /user.slice/user-1003.slice/user@1003.service
</span></span><span class=line><span class=cl>           ‚îú‚îÄapp.slice
</span></span><span class=line><span class=cl>           ‚îÇ ‚îî‚îÄapp-kollektivkart.slice
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îú‚îÄkollektivkart@8000.service
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ ‚îú‚îÄlibpod-payload-bb7340e633ea971aa8c57123e2909e40c744d2371ff70a8d2bba4a682244baa9
</span></span><span class=line><span class=cl>           ‚îÇ   ‚îÇ ‚îÇ ‚îî‚îÄ1871 /app/.venv/bin/python /app/.venv/bin/gunicorn kollektivkart.webapp:server --preload --bind 0.0.0.0:8000 --chdir<span class=o>=</span>/app --workers<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>Success! But so far, nobody can reach this service without using <code>ssh</code>. We&rsquo;ll need to configure caddy to proxy to these two instances.</p><h4 id=proxy-configuration>Proxy configuration<a hidden class=anchor aria-hidden=true href=#proxy-configuration>#</a></h4><p>Let&rsquo;s make a template for the configuration in <code>roles/kollektivkart/templates/kollektivkart.caddy.j2</code>:</p><pre tabindex=0><code>kollektivkart{{ env_suffix }}.kaveland.no {
    encode
    reverse_proxy {
       to localhost:8000 localhost:8001
       health_uri /ready
       health_interval 2s
       health_timeout 1s
       health_status 200
    }
    log
}
</code></pre><p>We&rsquo;re setting up caddy to proxy for both instances, relying on it to discover on the <code>/ready</code> HTTP-endpoint when the backend can receive traffic. We&rsquo;ll let it check every two seconds. If we take down a container, caddy should stop sending requests to it within a couple of seconds. This is a good enough quality of service for my hobby projects. We need to add rendering of the template to <code>roles/kollektivkart/tasks/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Configure reverse proxy</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/caddy/proxies.d/kollektivkart.caddy&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>src</span><span class=p>:</span><span class=w> </span><span class=l>kollektivkart.caddy.j2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=l>root</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0644&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>notify</span><span class=p>:</span><span class=w> </span><span class=l>reload caddy</span><span class=w>
</span></span></span></code></pre></div><h4 id=but-what-about-my-job>But what about my job?<a hidden class=anchor aria-hidden=true href=#but-what-about-my-job>#</a></h4><p>The same image contains a job that needs to run once per night to ingest new public transit data from <a href=https://data.entur.no>data.entur.no</a>. Once it&rsquo;s crunched the latest data, we need to restart the kollektivkart service. We already decided that losing requests for ~2 seconds at a time is fine. We&rsquo;ll put a small rolling-restart script on the server. Let&rsquo;s start by putting this file in <code>roles/kollektivkart/files/rolling-restart</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=cp>#!/usr/bin/env bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=nb>set</span> -euf -o pipefail
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;Perform rolling-restart of kollektivkart@8000 and kollektivkart@8001&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> port in <span class=m>8000</span> 8001<span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>  systemctl --user restart kollektivkart@<span class=nv>$port</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> attempt in <span class=k>$(</span>seq <span class=m>1</span> 20<span class=k>)</span><span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>    sleep 3<span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> curl -s -o /dev/null -w <span class=s2>&#34;&#34;</span> -f http://localhost:<span class=nv>$port</span>/ready<span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>      break<span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span>
</span></span><span class=line><span class=cl>      <span class=nb>echo</span> <span class=s2>&#34;kollektivkart@</span><span class=nv>$port</span><span class=s2> Not up after attempt number </span><span class=nv>$attempt</span><span class=s2>, sleeping&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>fi</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$attempt</span><span class=s2>&#34;</span> -eq <span class=m>20</span> <span class=o>]</span> <span class=o>&amp;&amp;</span> ! curl -s -o /dev/null -w <span class=s2>&#34;&#34;</span> -f http://localhost:<span class=nv>$port</span>/ready<span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>      <span class=nb>echo</span> <span class=s2>&#34;kollektivkart@</span><span class=nv>$port</span><span class=s2> failed to start after 20 attempts&#34;</span>
</span></span><span class=line><span class=cl>      <span class=nb>exit</span> <span class=m>1</span>
</span></span><span class=line><span class=cl>    <span class=k>fi</span>
</span></span><span class=line><span class=cl>  <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span></code></pre></div><p>We could easily generalize this, we&rsquo;ll do that later if we discover that we need to reuse it. Restarting kollektivkart generally takes about 10 seconds; our script gives up after 60. That should be fine. If the first container does not come back up after a restart, we&rsquo;ll be at half-capacity. That&rsquo;s fine; we have two containers for the redundancy, not the capacity. The script will fail in that case. We&rsquo;ll have to look into getting some sort of monitoring for that later.</p><p>Let&rsquo;s add it to the server by making a task in <code>roles/kollektivkart/tasks/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up rolling-restart script</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>copy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/rolling-restart&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>src</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;rolling-restart&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>group</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0744&#34;</span><span class=w>
</span></span></span></code></pre></div><p>Now we can configure a quadlet for our job in <code>roles/kollektivkart/tasks/main.yml</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up kollektivkart etl quadlet</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>copy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/.config/containers/systemd/kollektivkart-etl.container&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=l>kollektivkart</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0600&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>content</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Unit]
</span></span></span><span class=line><span class=cl><span class=sd>      Description=Kollektivkart ETL container
</span></span></span><span class=line><span class=cl><span class=sd>      After=network.target
</span></span></span><span class=line><span class=cl><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Container]
</span></span></span><span class=line><span class=cl><span class=sd>      Image=ghcr.io/kaaveland/bus-eta:latest
</span></span></span><span class=line><span class=cl><span class=sd>      Volume=/home/{{ user }}/bq_credentials:/bq_credentials:ro
</span></span></span><span class=line><span class=cl><span class=sd>      Environment=GOOGLE_APPLICATION_CREDENTIALS=/bq_credentials
</span></span></span><span class=line><span class=cl><span class=sd>      Entrypoint=/app/.venv/bin/python
</span></span></span><span class=line><span class=cl><span class=sd>      Exec=-m kollektivkart.etl ${PARQUET_LOCATION} --memory-limit-gb 3 --max-cpus 2
</span></span></span><span class=line><span class=cl><span class=sd>      AutoUpdate=registry
</span></span></span><span class=line><span class=cl><span class=sd>      EnvironmentFile=/home/{{ user }}/kollektivkart.conf
</span></span></span><span class=line><span class=cl><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Service]
</span></span></span><span class=line><span class=cl><span class=sd>      Type=oneshot
</span></span></span><span class=line><span class=cl><span class=sd>      # This critical line lets systemd find the ${PARQUET_LOCATION} env var from this file
</span></span></span><span class=line><span class=cl><span class=sd>      # so we can pass it on the command line in the block above
</span></span></span><span class=line><span class=cl><span class=sd>      EnvironmentFile=/home/{{ user }}/kollektivkart.conf
</span></span></span><span class=line><span class=cl><span class=sd>      SyslogIdentifier=kollektivkart-etl
</span></span></span><span class=line><span class=cl><span class=sd>      CPUQuota=200%
</span></span></span><span class=line><span class=cl><span class=sd>      MemoryMax=4G
</span></span></span><span class=line><span class=cl><span class=sd>      WorkingDirectory=/home/{{ user }}
</span></span></span><span class=line><span class=cl><span class=sd>      ExecStartPost=/home/{{ user }}/rolling-restart
</span></span></span><span class=line><span class=cl><span class=sd>      
</span></span></span><span class=line><span class=cl><span class=sd>      [Install]
</span></span></span><span class=line><span class=cl><span class=sd>      WantedBy=default.target</span><span class=w>
</span></span></span></code></pre></div><p>It may look strange that we&rsquo;re passing <code>--memory-limit-gb</code> and <code>--max-cpus</code>, but the reason for that is to inform <a href=https://duckdb.org>DuckDB</a> about how much capacity it has. Otherwise, it might detect all the CPU cores and try to use more resources than we&rsquo;ve allowed for it. <code>CPUQuota=200%</code> doesn&rsquo;t prevent it from seeing how many cores the machine has, it is only a scheduling guarantee. It probably wouldn&rsquo;t hurt to let DuckDB use 33% on each of our six cores, but it seems friendlier to let it use two whole ones. ü§ó</p><p>The job needs an extra environment variable compared to the webapp, the webapp never accesses BigQuery directly. The <code>[Service]</code> section has <code>ExecStartPost</code>. This is a somewhat strange name-choice, I think, for a command that is to be run once the script is done. So, this systemd unit will run a container once, then do the rolling restart. But nothing actually starts this container, so we have to take care of that too. We can use a systemd timer for this, let&rsquo;s write it:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up kollektivkart etl nightly timer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>copy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>dest</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/home/{{ user }}/.config/systemd/user/kollektivkart-etl.timer&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>owner</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>group</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;{{ user }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0600&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>content</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      [Unit]
</span></span></span><span class=line><span class=cl><span class=sd>      Description=Run Kollektivkart ETL nightly
</span></span></span><span class=line><span class=cl><span class=sd>      RefuseManualStart=false
</span></span></span><span class=line><span class=cl><span class=sd>      RefuseManualStop=false
</span></span></span><span class=line><span class=cl><span class=sd>      [Timer]
</span></span></span><span class=line><span class=cl><span class=sd>      OnCalendar=*-*-* 04:00:00
</span></span></span><span class=line><span class=cl><span class=sd>      RandomizedDelaySec=30m
</span></span></span><span class=line><span class=cl><span class=sd>      Persistent=true
</span></span></span><span class=line><span class=cl><span class=sd>      Unit=kollektivkart-etl.service
</span></span></span><span class=line><span class=cl><span class=sd>      
</span></span></span><span class=line><span class=cl><span class=sd>      [Install]
</span></span></span><span class=line><span class=cl><span class=sd>      WantedBy=timers.target</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Enable kollektivkart-etl</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>machinectl shell {{ user }}@ /bin/systemctl --user enable kollektivkart-etl.timer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Start kollektivkart-etl timer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>machinectl shell {{ user }}@ /bin/systemctl --user start kollektivkart-etl.timer</span><span class=w>
</span></span></span></code></pre></div><p>This is a lot like a cronjob. We&rsquo;ve set it to go off at 04:00, with a randomized delay of up to 30 minutes. It&rsquo;s also set to persistent, which means if the machine is off from 04:00‚Äì04:30, it&rsquo;ll decide to run this job once it boots. Let&rsquo;s check if this worked:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ssh -p2222 kollektivkart@localhost systemctl --user status kollektivkart-etl.timer
</span></span><span class=line><span class=cl>‚óè kollektivkart-etl.timer - Run Kollektivkart ETL nightly
</span></span><span class=line><span class=cl>     Loaded: loaded <span class=o>(</span>/home/kollektivkart/.config/systemd/user/kollektivkart-etl.timer<span class=p>;</span> enabled<span class=p>;</span> preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>     Active: active <span class=o>(</span>waiting<span class=o>)</span> since Tue 2025-05-13 23:16:21 CEST<span class=p>;</span> 2s ago
</span></span><span class=line><span class=cl>    Trigger: Wed 2025-05-14 04:07:08 CEST<span class=p>;</span> 9h left
</span></span><span class=line><span class=cl>   Triggers: ‚óè kollektivkart-etl.service
</span></span></code></pre></div><p>That looks good! The job itself won&rsquo;t actually work without the correct BigQuery or S3 credentials, but everything&rsquo;s configured now.</p><h2 id=setting-up-a-test-server>Setting up a test server<a hidden class=anchor aria-hidden=true href=#setting-up-a-test-server>#</a></h2><p>We actually have everything we need to make this come alive on the internet now. If you read this far, you have my undying respect. Maybe you learned something?</p><p>I&rsquo;m going to quickly clickops a server in hetzner and point api-test.kaveland.no and kollektivkart-test.kaveland.no to it, then see if everything comes up. I name my personal servers after whisky distilleries. This one is going to be called dalwhinnie.</p><h3 id=dns>DNS<a hidden class=anchor aria-hidden=true href=#dns>#</a></h3><p>I&rsquo;m setting up A and AAAA records for dalwhinnie.kaveland.no, and putting this in my hosts.ini:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ini data-lang=ini><span class=line><span class=cl><span class=k>[test]</span>
</span></span><span class=line><span class=cl><span class=na>dalwhinnie.kaveland.no</span>
</span></span><span class=line><span class=cl><span class=k>[test:vars]</span>
</span></span><span class=line><span class=cl><span class=na>ansible_ssh_user</span><span class=o>=</span><span class=s>root # will later need to change to admin after first time provisioning</span>
</span></span><span class=line><span class=cl><span class=na>ansible_become_user</span><span class=o>=</span><span class=s>root</span>
</span></span></code></pre></div><p>I&rsquo;ll make CNAME records for api-test.kaveland.no and kollektivkart-test.kaveland.no pointing to dalwhinnie.kaveland.no.</p><p>I&rsquo;m removing the <code>secrets.yml</code> from the playbook and running this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ansible-playbook -i hosts.ini --limit <span class=nb>test</span> --ask-vault-pass initialize.yml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e @~/code/infra/secrets.yml <span class=c1># reads secrets from the real vault</span>
</span></span></code></pre></div><p>It applies OK to the brand-new server. A few moments later, once caddy has gotten certificates:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -I https://api-test.kaveland.no/app/eugene/random.sql
</span></span><span class=line><span class=cl>HTTP/2 <span class=m>200</span>
</span></span><span class=line><span class=cl>alt-svc: <span class=nv>h3</span><span class=o>=</span><span class=s2>&#34;:443&#34;</span><span class=p>;</span> <span class=nv>ma</span><span class=o>=</span><span class=m>2592000</span>
</span></span><span class=line><span class=cl>server: Caddy
</span></span><span class=line><span class=cl>date: Tue, <span class=m>13</span> May <span class=m>2025</span> 17:22:50 GMT
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl -I https://kollektivkart-test.kaveland.no/
</span></span><span class=line><span class=cl>HTTP/2 <span class=m>200</span>
</span></span><span class=line><span class=cl>alt-svc: <span class=nv>h3</span><span class=o>=</span><span class=s2>&#34;:443&#34;</span><span class=p>;</span> <span class=nv>ma</span><span class=o>=</span><span class=m>2592000</span>
</span></span><span class=line><span class=cl>content-type: text/html<span class=p>;</span> <span class=nv>charset</span><span class=o>=</span>utf-8
</span></span><span class=line><span class=cl>date: Tue, <span class=m>13</span> May <span class=m>2025</span> 17:23:38 GMT
</span></span><span class=line><span class=cl>server: Caddy
</span></span><span class=line><span class=cl>server: gunicorn
</span></span><span class=line><span class=cl>content-length: <span class=m>4161</span>
</span></span></code></pre></div><p>And we&rsquo;re in business! Caddy works as advertised. It took a minute or so to get certificates. If you notice time-traveling in the timestamps here, don&rsquo;t worry. I&rsquo;m not making a paradox. I&rsquo;ve just rerun some commands above in the late evening. Didn&rsquo;t mean to spook you. üëª</p><p>I did a quick reboot here and verified that everything came up, and the kollektivkart-etl job started automatically. The <code>rolling-restart</code> script works well enough for my purposes (I observed about 2 seconds of downtime, as expected). I deleted the server afterward. I can trivially make a new one.</p><h2 id=static-assets>Static assets<a hidden class=anchor aria-hidden=true href=#static-assets>#</a></h2><p>I don&rsquo;t host static assets from the server, I rely on bunny.net for that. <a href=/posts/2025-04-20-deploying-to-bunnycdn>Read more here if you&rsquo;d like.</a> This costs around $1 a month, for much better worldwide performance than I could ever achieve on a single server. Totally worth it. Bunny also has a container hosting service that would be very suitable for <code>eugene-web</code>.</p><h2 id=monitoring>Monitoring<a hidden class=anchor aria-hidden=true href=#monitoring>#</a></h2><p>The way I&rsquo;ve set this up, I must expect reboots. At some point, my entire infrastructure will be down. Since I&rsquo;m planning on running only a single server and could move it around a bit, my best option here is to use something external.</p><p>I&rsquo;ve set up a <a href=https://kaveland.status.phare.io/>statuspage</a> with <a href=https://phare.io/>phare.io</a>. At my level of usage, this is a free service. It pings three different URLs I run every few minutes, and it will email me if they stay down for a while. This was super easy to set up, and works very well. I inadvertently tested this by disabling DNSSEC on my domain before getting rid of the DS record the other day ü´† Going to write about everything I learned about DNSSEC from that in the future!</p><p>Phare works fine for DNSSEC-related outages. Take my word for it. You can find a more harmless way to test it!</p><p>For things like my ETL-job, I&rsquo;ll make a URL on my page that returns some status code other than 200 if the data is stale, and phare.io will notify me if the job has had some issue. I don&rsquo;t have a plan right now for detecting that a rolling restart failed, but something will come to me.</p><p>For the moment, I do not have anything better than <code>/var/log/syslog</code> and <code>journalctl</code> for viewing logs, and <code>sar</code> for viewing server load and resource consumption over time. That will do for a while, I think, it&rsquo;s not like I get a lot of traffic.</p><h2 id=technical-debt-we-could-fix>Technical debt we could fix<a hidden class=anchor aria-hidden=true href=#technical-debt-we-could-fix>#</a></h2><p>The big one here feels like the proxy setup and the port numbers. Here&rsquo;s what we did wrong:</p><ul><li>Putting the proxy hostname in at the app level.</li><li>Putting the port numbers at the app level.</li></ul><p>The hostname and the port numbers are what caddy needs to know about. I think we should probably have made an <code>proxy-endpoint</code> role that could connect the two, something like this üßê</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>roles</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>base-install</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>eugene </span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>vars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=m>3000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>proxy-endpoint</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>vars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;api{{ env_suffix }}.kaveland.no&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>routes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/eugene</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=m>3000</span><span class=w>
</span></span></span></code></pre></div><p>We could always consider that later, but it feels like a lot of trouble right now. For such a small setup, the best solution is probably to just use one central Caddyfile.</p><p>We should also consider having more playbooks. <code>initialize.yml</code> should be more bare-bones than it is, possibly do only the security-related things. That way, we could let it override the initial username to log in with, since we disable ssh access for the <code>root</code> user later.</p><p>We&rsquo;ll definitely find a reason to generalize the <code>rolling-restart</code> script at some point, or perhaps replace it with something that uses the <a href=https://caddyserver.com/docs/api>caddy management api</a> to drain the backend before we restart it.</p><p>We&rsquo;ve put image tags directly in the quadlets. That&rsquo;s not a great idea. It means we need to edit complex and annoying files to roll back, possibly while under stress. We should probably put the tag somewhere it&rsquo;s easier to edit. Quadlet files can reference environment files, so this is easily doable.</p><h2 id=architectural-weakness>Architectural weakness<a hidden class=anchor aria-hidden=true href=#architectural-weakness>#</a></h2><p>I currently have my S3 bucket and my server in the same region in Hetzner. This is <em>only</em> okay because I can recreate the data perfectly from an external source. Normally I would advocate <em>very strongly</em> for having data be replicated to other regions. <a href=https://www.pcmag.com/news/ovhcloud-data-center-devastated-by-fire-entire-building-destroyed>Data centers can burn down</a>.</p><p>On the whole, there&rsquo;s not a lot of redundancy. If the machine is down, my things are down. The fact that making a new server is fast makes this point less painful. It is straightforward to scale to a bigger machine. It&rsquo;s possible to scale out to more machines too, but that requires tinkering more with DNS, TLS and load-balancing.</p><p>There&rsquo;s no persistent database in this setup. I believe I have the chops to set up a reasonably stable postgres installation with <a href=https://pgbackrest.org/>pgBackRest</a> that ships backups to <em>elsewhere</em> eventually. But hosting a database is no joke, this is something I would advise anyone to consider buying as a service. It might seem expensive, but it&rsquo;s not.</p><p>For hobby purposes, I think all of these are non-issues, but I would advise spending more time on a setup like this if I were running a business. Or perhaps accept the cost of buying managed services; there are many that are worth paying for.</p><h2 id=why-go-through-all-this-trouble>Why go through all this trouble?<a hidden class=anchor aria-hidden=true href=#why-go-through-all-this-trouble>#</a></h2><p>I could say that it was about the journey, not the destination. That wouldn&rsquo;t be wrong, exactly. I enjoy tinkering with Linux servers, and I feel like I learned a lot doing this exercise. As a developer, I find myself building on top of leaky abstractions all the time. It is good to know a thing or two about what&rsquo;s underneath whatever abstraction I deploy my software on. This is a fairly lean setup for a machine. I like that.</p><p>The liberty to host my stuff wherever I want is important to me right now. Being able to put everything on a Linux server with a simple ansible-playbook makes me very flexible. This kind of deployment is possible at almost any provider, and means I get to test all the european providers I&rsquo;m interested in.</p><p>There&rsquo;s a fairly large initial time investment, but that&rsquo;s mostly done now. Setting up a new container is going to be rapid and simple in the future, now that all the pieces are in place.</p><p>I&rsquo;m currently running on bare metal, so the price/performance ratio is hard to beat. I have 6 physical CPU cores, 64GB of RAM and 500GB of NVMe SSD at a very reasonable ‚Ç¨46.63/month, including VAT. For comparison, this costs significantly less than a 2VCPU 8GB RAM D2_V3 in Azure, and I have no risk of noisy neighbors impeding on my VCPU time. I have no excuses for writing adequately efficient SQL anymore. I must drop all indexes immediately.</p><p>If I decide that I have no need for bare metal, I&rsquo;ll go back to 4 VCPU, 8GB RAM and 80GB NVMe SSD at ‚Ç¨7.8/month. This is enough to run what I have right now, I just bought something bigger to force me to have bigger ambitions.</p><p>If it turns out that there are a lot of issues with running like this, I can find some managed k8s or container solution instead, and I wouldn&rsquo;t have lost anything but time. But the time already paid for itself with increased knowledge and the entertainment of learning new things üéìü•≥</p><h2 id=what-did-we-learn>What did we learn?<a hidden class=anchor aria-hidden=true href=#what-did-we-learn>#</a></h2><p>Here&rsquo;s a short list:</p><ul><li><code>podman</code> and <code>systemd</code> integrate very nicely now.</li><li>quadlet templates are incredibly powerful and elegant! Just the right level of abstraction for this kind of project.</li><li><code>Caddy</code> makes it very trivial to do TLS with letsencrypt.</li><li><code>unattended-upgrades</code> can take care of patching.</li><li><code>k8s</code> and hosting solutions like fly.io or heroku do <em>a lot</em> of heavy lifting for you. Heavy lifting can be healthy, though! It&rsquo;s good to understand some of the problems they solve.</li><li>There&rsquo;s a reason why people are paying good üí∏ to have all this stuff be someone else&rsquo;s problem.</li><li>Stateless backends are straightforward and pleasant to self-host üéâ</li><li>Ansible is still alive and kicking, and I even remember some of it. We have barely scratched the surface of what it can do. It&rsquo;s powerful software. I think it has a Hetzner module, and DNS-integration with bunny.net, so it could probably automate the last manual steps too.</li><li>Ansible and Vagrant are a very nice combination for locally developing server configuration.</li></ul><p>If you&rsquo;d like to self-host on your own server, but this setup looks intimidating and complex, I totally get that. You may want to check out options like <a href=https://coolify.io/>coolify</a> or <a href=https://github.com/Dokploy/dokploy>dokploy</a>.</p><h2 id=where-to-go-from-here>Where to go from here?<a hidden class=anchor aria-hidden=true href=#where-to-go-from-here>#</a></h2><p>If you want to play and tinker with this, <a href=https://github.com/kaaveland/fire-and-forget-linux>all the code</a> is available. I made some minor modifications to make it more convenient to get started, but if you read all the way here, you&rsquo;ll have no trouble finding your way. Proud of you! Give yourself a pat on the back on my behalf.</p><p>There&rsquo;s a list of things to try in the section about technical debt. Here are some more ideas:</p><ul><li>Maybe try running Caddy in a container?</li><li>Check if we can use <a href=https://github.com/containers/podman/blob/main/docs/tutorials/socket_activation.md>socket activation</a>! This seems like an almost magic way to pass sockets from the host to the container.</li><li>Set up a <a href=https://github.com/containers/podman/blob/main/docs/tutorials/basic_networking.md>podman network</a>!</li><li>Make an app and deploy it in <a href=https://www.hetzner.com/>Hetzner</a>, then move it to <a href=https://upcloud.com/>upcloud</a> or <a href=https://www.scaleway.com/en/>scaleway</a>. You can go anywhere now!</li></ul><p>If you&rsquo;d like to refer back to an earlier installment, I placed links here for your convenience:</p><ol><li><a href=/posts/2025-05-13-fire-and-forget-linux-p1>The first post</a> covers local development of linux server configuration and essentials.</li><li><a href=/posts/2025-05-14-fire-and-forget-linux-p2>The second post</a> covers installation of <a href=https://podman.io/>podman</a> and <a href=https://caddyserver.com/>caddy</a>. It concludes by deploying a very simple stateless webapp in a container.</li></ol><p>Writing this piece took a while. If you read all the way to the end, it would mean the world to me if you&rsquo;d let me know what you think and perhaps share it with someone. You can find some contact information on my <a href=https://kaveland.no>personal page</a>. If there&rsquo;s any interest, I might do more projects with a scope like this in the future.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://kaveland.no/tags/cloud/>Cloud</a></li><li><a href=https://kaveland.no/tags/linux/>Linux</a></li><li><a href=https://kaveland.no/tags/ops/>Ops</a></li><li><a href=https://kaveland.no/tags/cdn/>Cdn</a></li><li><a href=https://kaveland.no/tags/duckdb/>Duckdb</a></li><li><a href=https://kaveland.no/tags/caddy/>Caddy</a></li><li><a href=https://kaveland.no/tags/ansible/>Ansible</a></li></ul><nav class=paginav><a class=prev href=https://kaveland.no/posts/2025-05-28-turning-the-bus-sql/><span class=title>¬´ Prev</span><br><span>Using SQL to turn all the buses around</span>
</a><a class=next href=https://kaveland.no/posts/2025-05-14-fire-and-forget-linux-p2/><span class=title>Next ¬ª</span><br><span>No-ops linux part 2: Hosting a simple container on a lean mean systemd machine</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on x" href="https://x.com/intent/tweet/?text=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly.&amp;url=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f&amp;hashtags=cloud%2clinux%2cops%2ccdn%2cduckdb%2ccaddy%2cansible"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f&amp;title=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly.&amp;summary=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly.&amp;source=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f&title=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly."><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on whatsapp" href="https://api.whatsapp.com/send?text=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly.%20-%20https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on telegram" href="https://telegram.me/share/url?text=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly.&amp;url=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share No-ops linux part 3: It puts the data in the pond. Nightly. on ycombinator" href="https://news.ycombinator.com/submitlink?t=No-ops%20linux%20part%203%3a%20It%20puts%20the%20data%20in%20the%20pond.%20Nightly.&u=https%3a%2f%2fkaveland.no%2fposts%2f2025-05-14-fire-and-forget-linux-p3%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://kaveland.no/>Robin's blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>